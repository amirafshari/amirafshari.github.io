"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[7488],{1872:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>t,default:()=>m,frontMatter:()=>r,metadata:()=>l,toc:()=>c});var s=a(7624),i=a(5788);const r={slug:"remote-sensing-image-search-cbir-becnchmark",title:"Satellite Data Vector Database",authors:{name:"Amir Afshari",title:"Machine Learning Engineer",url:"https://github.com/amirafshari",image_url:"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4"},tags:["Deep Learning","Machine Learning","Computer Vision","Remote Sensing"]},t=void 0,l={permalink:"/blog/remote-sensing-image-search-cbir-becnchmark",editUrl:"https://github.com/amirafshari/amirafshari.github.io/tree/main/blog/2024-05-18-remote-sensing-image-search-benchmark.md",source:"@site/blog/2024-05-18-remote-sensing-image-search-benchmark.md",title:"Satellite Data Vector Database",description:"png",date:"2024-05-18T00:00:00.000Z",formattedDate:"May 18, 2024",tags:[{label:"Deep Learning",permalink:"/blog/tags/deep-learning"},{label:"Machine Learning",permalink:"/blog/tags/machine-learning"},{label:"Computer Vision",permalink:"/blog/tags/computer-vision"},{label:"Remote Sensing",permalink:"/blog/tags/remote-sensing"}],readingTime:13.96,hasTruncateMarker:!0,authors:[{name:"Amir Afshari",title:"Machine Learning Engineer",url:"https://github.com/amirafshari",image_url:"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4",imageURL:"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4"}],frontMatter:{slug:"remote-sensing-image-search-cbir-becnchmark",title:"Satellite Data Vector Database",authors:{name:"Amir Afshari",title:"Machine Learning Engineer",url:"https://github.com/amirafshari",image_url:"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4",imageURL:"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4"},tags:["Deep Learning","Machine Learning","Computer Vision","Remote Sensing"]},unlisted:!1,nextItem:{title:"Multimodal Vision-Language Search on Satellite Images",permalink:"/blog/remote-sensing-image-search-cbir"}},o={authorsImageUrls:[void 0]},c=[{value:"Solution",id:"solution",level:3},{value:"Good to know",id:"good-to-know",level:3},{value:"Datasets",id:"datasets",level:2},{value:"AID",id:"aid",level:3},{value:"FAIR1M",id:"fair1m",level:3},{value:"RESISC45",id:"resisc45",level:3},{value:"Sentinel-2 ship",id:"sentinel-2-ship",level:3},{value:"Models",id:"models",level:2},{value:"Swin",id:"swin",level:3},{value:"ResNet",id:"resnet",level:3},{value:"RegNet",id:"regnet",level:3},{value:"Metrics",id:"metrics",level:2},{value:"Vector Database",id:"vector-database",level:2},{value:"NN Search",id:"nn-search",level:2},{value:"Evaluation",id:"evaluation",level:2},{value:"Accuracies Confusion Matrix",id:"accuracies-confusion-matrix",level:3},{value:"Predictions Confusion Matrix",id:"predictions-confusion-matrix",level:3}];function d(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.MN)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:a(2124).c+"",width:"800",height:"500"})}),"\n",(0,s.jsx)(n.p,{children:"With the rapid advancement of remote sensing technologies, the availability of large-scale satellite image datasets has grown exponentially. These datasets contain invaluable information for various applications, including environmental monitoring, urban planning, and disaster management. However, extracting specific categories of objects, such as identifying all images which are similar to a sepcific query image\nwithin a dataset of thousands or millions or even billions of samples, presents a significant challenge for human analyst.\nships within a dataset of one million samples, presents a significant challenge due to the sheer volume of data and the complexity of manual analysis."}),"\n",(0,s.jsx)(n.h3,{id:"solution",children:"Solution"}),"\n",(0,s.jsx)(n.p,{children:"This task, which is overwhelming for human analysts, can be efficiently addressed using vector search techniques. By leveraging deep learning models to transform images into high-dimensional vectors and utilizing various models such as classification, segmentation, etc, we can use their last layer features and employ nearest neighbor search algorithms to quickly and accurately retrieve relevant images based on their content or semantic meaning."}),"\n",(0,s.jsx)(n.p,{children:"For instance, you find and interesting shape in your dataset and you want to figure out if there is any similar image in your dataset or not? To do so, you can use that image as a search query to find the similar images."}),"\n",(0,s.jsx)(n.h3,{id:"good-to-know",children:"Good to know"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"I just fine-tuned ResNet50 on AID dataset, we can train more architectures on different datasets and benchmark them."}),"\n",(0,s.jsx)(n.li,{children:"By fine-tuning on a large-scale dataset, the accuracy significantly improves."}),"\n",(0,s.jsx)(n.li,{children:"It's not a production-level (or even near-production-level) solution, so there is plenty of room for improvement in both speed and accuracy."}),"\n",(0,s.jsx)(n.li,{children:"We can use vector databases such as Weaviate or Redis instead of a simple Python list."}),"\n",(0,s.jsx)(n.li,{children:"It's good to dig into the datasets first and then judge the performance of the model."}),"\n",(0,s.jsxs)(n.li,{children:["You can find the ",(0,s.jsx)(n.a,{href:"https://github.com/amirafshari/rs-cbir",children:"code on Github"})]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pandas as pd\nimport numpy as np \nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms, utils\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom skimage import io, transform\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nimport os\nimport glob\nimport random\n\nfrom PIL import Image\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\ndevice\n'})}),"\n",(0,s.jsx)(n.p,{children:"device(type='cuda')"}),"\n",(0,s.jsx)(n.h2,{id:"datasets",children:"Datasets"}),"\n",(0,s.jsx)(n.h3,{id:"aid",children:"AID"}),"\n",(0,s.jsx)(n.p,{children:"To Do"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Dataset Details"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class AID(Dataset):\n\n    def __init__(self, root_dir):\n\n        self.label_map = {}\n        self.info = []\n        i = 0\n        for root, dirs, files in os.walk(root_dir):\n                \n            for file in files:\n                \n                if file.endswith((".jpg", ".tif", ".png", "jpeg")):\n                    file_path = os.path.join(root, file)\n                    label = root.split(\'/\')[-1]\n                    self.info.append((file_path, label.lower()))\n\n\n                if label.lower() not in self.label_map:\n                    self.label_map[label.lower()] = i\n                    i += 1\n\n        random.shuffle(self.info)\n        self.transform = transform\n\n\n    def map_labels(self, label):\n        return self.label_map[label]\n\n\n\n\n\n    def __len__(self):\n        return len(self.info)\n\n\n    def __getitem__(self, idx):\n        img_path, label = self.info[idx]\n\n        label = self.map_labels(label.lower())\n\n        img = io.imread(img_path)[:,:,:3]\n        img = img / 255.\n        img = torch.tensor(img, dtype=torch.float32)\n        \n        return img, label, img_path\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"dataset = AID('datasets/AID')\n\nlabel_map = dataset.label_map\nclass_map = {v: k for k, v in label_map.items()}\nclasses = [v for k,v in class_map.items()]\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"rows, columns = 2, 5\nfig, axs = plt.subplots(rows, columns, figsize=(21, 8), squeeze=True)\n\n\nn = 0\nfor i in range(rows):\n    for j in range(columns):\n        img, label, _ = dataset[n]\n        clss = class_map[label]\n\n        axs[i][j].imshow(img)\n        axs[i][j].set_title(clss)\n        n += 1\nplt.show()\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:a(3764).c+"",width:"800",height:"300"})}),"\n",(0,s.jsx)(n.h3,{id:"fair1m",children:"FAIR1M"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class FAIR1M(Dataset):\n\n    def __init__(self, root_dir, transform=None):\n\n        self.label_map = {\n            'ship': 0,\n            'airplane': 1,\n            'neighborhood': 2,\n        }\n\n        self.info = []\n        for root, dirs, files in os.walk(root_dir):\n            for file in files:\n                if file.endswith((\".jpg\", \".tif\", \".png\", \"jpeg\")):\n                    file_path = os.path.join(root, file)\n                    label = root.split('/')[-1]\n                    self.info.append((file_path, label.lower()))\n\n        random.shuffle(self.info)\n        self.transform = transform\n\n    def map_labels(self, label):\n        return self.label_map[label]\n\n\n\n\n\n    def __len__(self):\n        return len(self.info)\n\n\n    def __getitem__(self, idx):\n        img_path, label = self.info[idx]\n\n        label = self.map_labels(label.lower())\n\n        img = io.imread(img_path)[:,:,:3]\n        img = img / 255.\n        img = torch.tensor(img, dtype=torch.float32)\n        \n        return img, label, img_path\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"dataset = FAIR1M('datasets/FAIR1M_partial')\n\nclass_map = {\n    0: 'ship',\n    1: 'airplane',\n    2: 'neighborhood',\n}\nclasses = [v for k,v in class_map.items()]\nclasses\n"})}),"\n",(0,s.jsx)(n.p,{children:"['ship', 'airplane', 'neighborhood']"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"rows, columns = 2, 5\nfig, axs = plt.subplots(rows, columns, figsize=(21, 8), squeeze=True)\n\n\nn = 0\nfor i in range(rows):\n    for j in range(columns):\n        img, label, _ = dataset[n]\n        clss = class_map[label]\n\n        axs[i][j].imshow(img)\n        axs[i][j].set_title(clss)\n        n += 1\nplt.show()\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:a(9572).c+"",width:"800",height:"300"})}),"\n",(0,s.jsx)(n.h3,{id:"resisc45",children:"RESISC45"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class RESISC45(Dataset):\n\n    def __init__(self, root_dir, transform=None):\n\n        self.label_map = {\n            'ship': 0,\n            'airplane': 1,\n            'bridge': 2,\n        }\n\n        self.info = []\n        for root, dirs, files in os.walk(root_dir):\n            for file in files:\n                if file.endswith((\".jpg\", \".tif\", \".png\", \"jpeg\")):\n                    file_path = os.path.join(root, file)\n                    label = root.split('/')[-1]\n                    self.info.append((file_path, label))\n\n        random.shuffle(self.info)\n        self.transform = transform\n\n    def map_labels(self, label):\n        return self.label_map[label]\n\n\n\n\n\n    def __len__(self):\n        return len(self.info)\n\n\n    def __getitem__(self, idx):\n        img_path, label = self.info[idx]\n\n        label = self.map_labels(label.lower())\n\n        img = io.imread(img_path)[:,:,:3]\n        img = img / 255.\n        img = torch.tensor(img, dtype=torch.float32)\n        \n        return img, label, img_path\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"dataset = RESISC45('datasets/RESISC45_partial')\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class_map = {\n    0: 'ship',\n    1: 'airplane',\n    2: 'bridge',\n}\nclasses = [v for k,v in class_map.items()]\nclasses\n"})}),"\n",(0,s.jsx)(n.p,{children:"['ship', 'airplane', 'bridge']"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"rows, columns = 2, 5\nfig, axs = plt.subplots(rows, columns, figsize=(21, 8), squeeze=True)\n\n\nn = 0\nfor i in range(rows):\n    for j in range(columns):\n        img, label, _ = dataset[n]\n        clss = class_map[label]\n\n        axs[i][j].imshow(img)\n        axs[i][j].set_title(clss)\n        n += 1\nplt.show()\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:a(6868).c+"",width:"800",height:"300"})}),"\n",(0,s.jsx)(n.h3,{id:"sentinel-2-ship",children:"Sentinel-2 ship"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class SS2(Dataset):\n\n    def __init__(self, root_dir, transform=None):\n\n        self.label_map = {\n            \'ship\': 0,\n            \'noship\': 1,\n        }\n\n        self.info = []\n        for root, dirs, files in os.walk(root_dir):\n            for file in files:\n                if file.endswith((".jpg", ".tif", ".png", "jpeg")):\n                    file_path = os.path.join(root, file)\n                    label = root.split(\'/\')[-1]\n                    self.info.append((file_path, label))\n\n        random.shuffle(self.info)\n        self.transform = transform\n\n    def map_labels(self, label):\n        return self.label_map[label]\n\n\n\n\n\n    def __len__(self):\n        return len(self.info)\n\n\n    def __getitem__(self, idx):\n        img_path, label = self.info[idx]\n\n        label = self.map_labels(label.lower())\n\n        img = io.imread(img_path)[:,:,:3]\n        img = img / 255.\n        img = torch.tensor(img, dtype=torch.float32)\n        \n        return img, label, img_path\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"dataset = SS2('datasets/Sentinel2_partial')\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class_map = {\n    0: 'ship',\n    1: 'noship',\n}\nclasses = [v for k,v in class_map.items()]\nclasses\n"})}),"\n",(0,s.jsx)(n.p,{children:"['ship', 'noship']"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"rows, columns = 2, 5\nfig, axs = plt.subplots(rows, columns, figsize=(21, 8), squeeze=True)\n\n\nn = 0\nfor i in range(rows):\n    for j in range(columns):\n        img, label, _ = dataset[n]\n        clss = class_map[label]\n\n        axs[i][j].imshow(img)\n        axs[i][j].set_title(clss)\n        n += 1\nplt.show()\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:a(6020).c+"",width:"800",height:"300"})}),"\n",(0,s.jsx)(n.h2,{id:"models",children:"Models"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://pytorch.org/vision/stable/models.html",children:"https://pytorch.org/vision/stable/models.html"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://pytorch.org/vision/main/models",children:"https://pytorch.org/vision/main/models"})}),"\n",(0,s.jsx)(n.li,{children:"load your neural netwrok for feature extraction"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"To Do"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use fine-tuned weights"}),"\n",(0,s.jsx)(n.li,{children:"Use different architectures for this task and compare their infernce speed and accuracy."}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"weights = ['IMAGENET1K_V1', 'IMAGENET1K_V2', 'IMAGENET1K_SWAG_E2E_V1', 'IMAGENET1K_SWAG_E2E_V1', 'IMAGENET1K_SWAG_LINEAR_V1']\n"})}),"\n",(0,s.jsx)(n.p,{children:"This is a simple way to load a pre-trained model using PyTorch"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'model = torch.hub.load("pytorch/vision", \'resnet50\', weights="IMAGENET1K_V2")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"swin",children:"Swin"}),"\n",(0,s.jsx)(n.p,{children:"Pre-trained on ImageNet"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Swin\nfrom torchvision.models import swin_v2_b, Swin_B_Weights\n\nmodel = models.swin_v2_b(weights=Swin_B_Weights).to(device).eval()\n\ntotal_params = '{:,}'.format(sum(p.numel() for p in model.parameters()))\ntotal_params\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"model\n"})}),"\n",(0,s.jsx)(n.h3,{id:"resnet",children:"ResNet"}),"\n",(0,s.jsx)(n.p,{children:"Pre-trained on ImageNet"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2).to(device).eval()\n# model = models.resnet50(pretrained=True)\n"})}),"\n",(0,s.jsx)(n.p,{children:"Fine-tuned on AID"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"model = models.resnet50(pretrained=False)\n"})}),"\n",(0,s.jsxs)(n.p,{children:["/home/amir/miniconda3/envs/faiss/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\nwarnings.warn(\n/home/amir/miniconda3/envs/faiss/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or ",(0,s.jsx)(n.code,{children:"None"})," for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing ",(0,s.jsx)(n.code,{children:"weights=None"}),".\nwarnings.warn(msg)"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"model\n"})}),"\n",(0,s.jsx)(n.p,{children:"Modify the last layer based on your fine-tuned weights"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"num_classes = 17 # AID\n# num_classes = 43 # BigEarthNet\n\nmodel.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"checkpoint = torch.load('weights/aid_multilabel_scratch_resnet50.pth', map_location=torch.device('cpu'))\nstate_dict = checkpoint['state_dict']\nmodel.load_state_dict(state_dict)\nmodel.to(device).eval()\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"    ResNet(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n      (fc): Linear(in_features=2048, out_features=17, bias=True)\n    )\n"})}),"\n",(0,s.jsx)(n.p,{children:"Using this module, we can extract features from the last layer of our network, to use it as our feature extractor for our vector database"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class ResNetFeatures(torch.nn.Module):\n    def __init__(self, original_model):\n        super(ResNetFeatures, self).__init__()\n        self.features = torch.nn.Sequential(*list(original_model.children())[:-1])  # all layers except the final FC layer\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        return x\n\n# Create an instance of the new model\nmodel = ResNetFeatures(model).to(device).eval()\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"for key in state_dict.keys():\n    print(key)\n"})}),"\n",(0,s.jsx)(n.p,{children:"Number of model parameters"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"total_params = '{:,}'.format(sum(p.numel() for p in model.parameters()))\ntotal_params\n"})}),"\n",(0,s.jsx)(n.p,{children:"'23,508,032'"}),"\n",(0,s.jsx)(n.h3,{id:"regnet",children:"RegNet"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# RegNet\n\nmodel = models.regnet_y_400mf(pretrained=True).to(device).eval()\n\ntotal_params = '{:,}'.format(sum(p.numel() for p in model.parameters()))\ntotal_params\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"model\n"})}),"\n",(0,s.jsx)(n.h2,{id:"metrics",children:"Metrics"}),"\n",(0,s.jsx)(n.p,{children:"To Do"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"What metrics for CBIR?"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def accuracy(query_label, neighbors, labels):\n    ''' A simple function to calculate the accuracy for 1 sample @ K'''\n    t, f = 0, 0\n    for i in neighbors:\n        if query_label == labels[i]:\n            t += 1\n        else:\n            f += 1\n    return '{:.1%}'.format(t / (t + f))\n"})}),"\n",(0,s.jsx)(n.h2,{id:"vector-database",children:"Vector Database"}),"\n",(0,s.jsx)(n.p,{children:"Here we use our feature extractor to convert all of our images to a (1, 2048) vector and store them in our vector database (a python list in here) and perform our NN search."}),"\n",(0,s.jsx)(n.p,{children:"Of course it's not implemented for production level use, but we can scale it using Vector Database such as Weaviate or Pinecone, etc. We can also use simpler implementation with FAISS library for our vector database and NN search."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"features = []\nimage_paths = []\nlabels = []\n\n\n# Change the first line according to the dataset you want to use\nfor x, y, img_path in dataset:\n    img = x.permute(2, 0, 1).unsqueeze(0)\n    \n    with torch.no_grad():\n        feature = model(img.to(device)).detach().cpu().numpy()\n\n    features.append(feature)\n    image_paths.append(img_path)\n    labels.append(y)\n\n\nfeatures = np.concatenate(features, axis=0)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"features.shape\n"})}),"\n",(0,s.jsx)(n.p,{children:"(10000, 2048)"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Swin"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"took"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"ResNet"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Took 4m 20s for AID on NVIDIA 3060"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"nn-search",children:"NN Search"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"id = random.randint(0, len(dataset))\nimg, label, img_path = dataset[id]\nimg = img.permute(2, 0, 1).unsqueeze(0)\n\nquery_image_path = img_path\nquery_label = label\n\nwith torch.no_grad():\n    query_feature = model(img.to(device)).detach().cpu().numpy()\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"k = 20\n\nneigh = NearestNeighbors(n_neighbors=k, algorithm='brute')\nneigh.fit(features)\ndistances, indices = neigh.kneighbors(query_feature)\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Plot query image\nquery_image = Image.open(query_image_path)\n\n\nprint(f'Accuracy for this sample @ k = {k}: {accuracy(query_label, indices[0], labels)}')\n\nrows, columns = 4, 5\nfig, axs = plt.subplots(rows, columns, figsize=(34, 21))\n\nn = 0\nfor i in range(rows):\n    for j in range(columns):\n        if (i == 0) and (j == 0):\n            axs[0][0].imshow(query_image)\n            axs[0][0].set_title(f'Query: {class_map[query_label]}')\n        else:\n            axs[i][j].imshow(Image.open(image_paths[indices[0][n+1]]))\n            axs[i][j].set_title(f'NN {n+1}: {class_map[labels[indices[0][n+1]]]}')\n            n += 1\n"})}),"\n",(0,s.jsx)(n.p,{children:"Accuracy for this sample @ k = 20: 100.0%"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:a(2124).c+"",width:"800",height:"500"})}),"\n",(0,s.jsx)(n.h2,{id:"evaluation",children:"Evaluation"}),"\n",(0,s.jsx)(n.p,{children:"We used the AID dataset for both training and inference"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def compute_confusion_matrix_and_accuracy(dataset, model, device, features, labels, ks):\n    # Initialize a dictionary to hold confusion matrices and accuracies for different k\n    results = {k: {'confusion_matrix': None, 'accuracy': None, 'per_class_accuracy': None, 'confusion_matrix_accuracy': None} for k in ks}\n    \n    # Initialize the NearestNeighbors model\n    neigh = NearestNeighbors(n_neighbors=max(ks), algorithm='brute')\n    neigh.fit(features)\n    \n    # Prepare ground truth and predicted labels for confusion matrix calculation\n    all_true_labels = []\n    all_pred_labels = {k: [] for k in ks}\n    \n    for i in range(len(dataset)):\n        img, label, _ = dataset[i]\n        img = img.permute(2, 0, 1).unsqueeze(0)\n\n        with torch.no_grad():\n            query_feature = model(img.to(device)).detach().cpu().numpy()\n\n        distances, indices = neigh.kneighbors(query_feature)\n        \n        true_label = labels[i]\n        all_true_labels.append(true_label)\n        \n        for k in ks:\n            neighbors = indices[0][:k]\n            predicted_label = np.bincount([labels[n] for n in neighbors]).argmax()\n            all_pred_labels[k].append(predicted_label)\n    \n    # Calculate confusion matrices and accuracies for each k\n    for k in ks:\n        cm = confusion_matrix(all_true_labels, all_pred_labels[k])\n        acc = accuracy_score(all_true_labels, all_pred_labels[k])\n\n        results[k]['confusion_matrix'] = cm\n        results[k]['accuracy'] = acc\n        # results[k]['per_class_accuracy'] = cm.diagonal() / cm.sum(axis=0)\n\n        conf_mat_acc = np.zeros_like(cm, dtype=float)\n        # results[k]['confusion_matrix'].dtype = float\n        for i, row in enumerate(cm):\n            conf_mat_acc[i] = row / row.sum()\n        results[k]['confusion_matrix_accuracy'] = conf_mat_acc\n    \n\n    \n    return results\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def visualize_confusion_matrix(cm, class_names, k):\n    plt.figure(figsize=(18, 18))  # Increase figure size\n    sns.set(font_scale=1.2)  # Increase font scale\n    sns.heatmap(cm, annot=True, fmt='', cmap='Blues', xticklabels=class_names, yticklabels=class_names, annot_kws={\"size\": 12, \"ha\": 'center', \"va\": 'center'}, linewidths=.5, linecolor='black')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.title(f'Predictions for k={k}')\n    plt.show()\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"ks = [10, 50, 100]\nresults = compute_confusion_matrix_and_accuracy(dataset, model, device, features, labels, ks)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"accuracies-confusion-matrix",children:"Accuracies Confusion Matrix"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"for k in ks:\n    visualize_confusion_matrix(np.round(results[k]['confusion_matrix_accuracy'], 2), classes, k)\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:a(7100).c+"",width:"800",height:"800"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:a(9296).c+"",width:"800",height:"800"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:a(3024).c+"",width:"800",height:"800"})}),"\n",(0,s.jsx)(n.h3,{id:"predictions-confusion-matrix",children:"Predictions Confusion Matrix"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"for k in ks:\n    visualize_confusion_matrix(results[k]['confusion_matrix'], classes, k)\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:a(5080).c+"",width:"800",height:"800"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:a(4232).c+"",width:"800",height:"800"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:a(2156).c+"",width:"800",height:"800"})})]})}function m(e={}){const{wrapper:n}={...(0,i.MN)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},5788:(e,n,a)=>{a.d(n,{MN:()=>c});var s=a(1504);function i(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function r(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);n&&(s=s.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,s)}return a}function t(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?r(Object(a),!0).forEach((function(n){i(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function l(e,n){if(null==e)return{};var a,s,i=function(e,n){if(null==e)return{};var a,s,i={},r=Object.keys(e);for(s=0;s<r.length;s++)a=r[s],n.indexOf(a)>=0||(i[a]=e[a]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(s=0;s<r.length;s++)a=r[s],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var o=s.createContext({}),c=function(e){var n=s.useContext(o),a=n;return e&&(a="function"==typeof e?e(n):t(t({},n),e)),a},d={inlineCode:"code",wrapper:function(e){var n=e.children;return s.createElement(s.Fragment,{},n)}},m=s.forwardRef((function(e,n){var a=e.components,i=e.mdxType,r=e.originalType,o=e.parentName,m=l(e,["components","mdxType","originalType","parentName"]),u=c(a),h=i,p=u["".concat(o,".").concat(h)]||u[h]||d[h]||r;return a?s.createElement(p,t(t({ref:n},m),{},{components:a})):s.createElement(p,t({ref:n},m))}));m.displayName="MDXCreateElement"},9572:(e,n,a)=>{a.d(n,{c:()=>s});const s=a.p+"assets/images/image-search_13_0-13d2af7f5f68dc6d50f351728fd8ddf6.png"},6868:(e,n,a)=>{a.d(n,{c:()=>s});const s=a.p+"assets/images/image-search_18_0-12cbcd9abf2444e3218ae01195aabf52.png"},6020:(e,n,a)=>{a.d(n,{c:()=>s});const s=a.p+"assets/images/image-search_23_0-09e7da946801ed88c33a4bfde78c9efc.png"},2124:(e,n,a)=>{a.d(n,{c:()=>s});const s=a.p+"assets/images/image-search_60_1-640dab6c0bee7b9b33e6a35412c6dd34.png"},7100:(e,n,a)=>{a.d(n,{c:()=>s});const s=a.p+"assets/images/image-search_66_0-63ce4112af9273a7a683d2b1bd74290c.png"},9296:(e,n,a)=>{a.d(n,{c:()=>s});const s=a.p+"assets/images/image-search_66_1-295bc34f26651e527ead3f3151ee24e8.png"},3024:(e,n,a)=>{a.d(n,{c:()=>s});const s=a.p+"assets/images/image-search_66_2-f3005aba9d4fc86cbf7137f7bf44efc3.png"},5080:(e,n,a)=>{a.d(n,{c:()=>s});const s=a.p+"assets/images/image-search_68_0-e6821aa1e926207e49eb3f59f0410da2.png"},4232:(e,n,a)=>{a.d(n,{c:()=>s});const s=a.p+"assets/images/image-search_68_1-5c43d7105b41e4da8500c2632af29516.png"},2156:(e,n,a)=>{a.d(n,{c:()=>s});const s=a.p+"assets/images/image-search_68_2-61146bbdcc9dfa033d6d0abb7213cbaf.png"},3764:(e,n,a)=>{a.d(n,{c:()=>s});const s=a.p+"assets/images/image-search_8_0-25c11ee343a89a4df15c21a610eb0aff.png"}}]);