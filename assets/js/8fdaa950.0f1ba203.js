"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6104],{4692:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var i=a(7624),t=a(5788);const s={slug:"remote-sensing-image-search-cbir",title:"Multimodal Vision-Language Search on Satellite Images",authors:{name:"Amir Afshari",title:"Machine Learning Engineer",url:"https://github.com/amirafshari",image_url:"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4"},tags:["Deep Learning","Machine Learning","Computer Vision","Remote Sensing"]},r=void 0,o={permalink:"/blog/remote-sensing-image-search-cbir",editUrl:"https://github.com/amirafshari/amirafshari.github.io/tree/main/blog/2024-05-18-remote-sensing-image-search.md",source:"@site/blog/2024-05-18-remote-sensing-image-search.md",title:"Multimodal Vision-Language Search on Satellite Images",description:"png",date:"2024-05-18T00:00:00.000Z",formattedDate:"May 18, 2024",tags:[{label:"Deep Learning",permalink:"/blog/tags/deep-learning"},{label:"Machine Learning",permalink:"/blog/tags/machine-learning"},{label:"Computer Vision",permalink:"/blog/tags/computer-vision"},{label:"Remote Sensing",permalink:"/blog/tags/remote-sensing"}],readingTime:2.745,hasTruncateMarker:!0,authors:[{name:"Amir Afshari",title:"Machine Learning Engineer",url:"https://github.com/amirafshari",image_url:"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4",imageURL:"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4"}],frontMatter:{slug:"remote-sensing-image-search-cbir",title:"Multimodal Vision-Language Search on Satellite Images",authors:{name:"Amir Afshari",title:"Machine Learning Engineer",url:"https://github.com/amirafshari",image_url:"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4",imageURL:"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4"},tags:["Deep Learning","Machine Learning","Computer Vision","Remote Sensing"]},unlisted:!1,prevItem:{title:"Satellite Data Vector Database",permalink:"/blog/remote-sensing-image-search-cbir-becnchmark"},nextItem:{title:"Automatic License Plate Detector",permalink:"/blog/license-plate-detector"}},l={authorsImageUrls:[void 0]},c=[{value:"Solution",id:"solution",level:3},{value:"Good to know",id:"good-to-know",level:3},{value:"Model",id:"model",level:3},{value:"Image Embedding",id:"image-embedding",level:3},{value:"Dataset Visualization",id:"dataset-visualization",level:3},{value:"NN Search",id:"nn-search",level:3}];function d(e){const n={a:"a",code:"code",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,t.MN)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"png",src:a(3604).c+"",width:"800",height:"800"})}),"\n",(0,i.jsx)(n.p,{children:"With the rapid advancement of remote sensing technologies, the availability of large-scale satellite image datasets has grown exponentially. These datasets contain invaluable information for various applications, including environmental monitoring, urban planning, and disaster management. However, extracting specific categories of objects, such as identifying all ships within a dataset of one million samples, presents a significant challenge due to the sheer volume of data and the complexity of manual analysis."}),"\n",(0,i.jsx)(n.h3,{id:"solution",children:"Solution"}),"\n",(0,i.jsx)(n.p,{children:'This task, which is overwhelming for human analysts, can be efficiently addressed using vector search techniques. By leveraging deep learning models to transform images into high-dimensional vectors and utilizing multimodal vision-text models such as CLIP, we can employ nearest neighbor search algorithms to quickly and accurately retrieve relevant images based on their content. For instance, you can search for "red ship floating on the sea" and use it as the query and the system provides you the appropriate instances of the dataset while there is no metadata available. This approach not only enhances the efficiency of data processing but also significantly improves speed of finding specific categories within vast datasets.'}),"\n",(0,i.jsx)(n.h3,{id:"good-to-know",children:"Good to know"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"I did not fine-tune CLIP for remote sensing text-image pairs, but it still works fine."}),"\n",(0,i.jsx)(n.li,{children:"By fine-tuning, we can enhance the accuracy."}),"\n",(0,i.jsx)(n.li,{children:"It's not a production-level (or even near-production-level) solution, so there is plenty of room for improvement in both speed and accuracy."}),"\n",(0,i.jsx)(n.li,{children:"We can use vector databases such as Weaviate or Redis instead of a simple Python list."}),"\n",(0,i.jsx)(n.li,{children:"The dataset is limited to approximately 14,000 samples (a combination of AID, FAIR1M_partial, RESICS_partial, ...)."}),"\n",(0,i.jsx)(n.li,{children:"We can binarize the vectors to improve speed."}),"\n",(0,i.jsxs)(n.li,{children:["You can find the ",(0,i.jsx)(n.a,{href:"https://github.com/amirafshari/rs-cbir",children:"code on Github"})]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nfrom torchvision import models, transforms\nfrom sklearn.neighbors import NearestNeighbors\nimport clip\n\n\nimport random\nimport os\nimport glob\n\nfrom PIL import Image\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\ndevice\n'})}),"\n",(0,i.jsx)(n.p,{children:"device(type='cuda')"}),"\n",(0,i.jsx)(n.h3,{id:"model",children:"Model"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/mlfoundations/open_clip",children:"https://github.com/mlfoundations/open_clip"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/openai/CLIP/blob/main/README.md",children:"https://github.com/openai/CLIP/blob/main/README.md"})}),"\n",(0,i.jsx)(n.li,{children:"CLIP"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'model, preprocess = clip.load("ViT-B/32", device=device)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"image-embedding",children:"Image Embedding"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Walk into directoies to find images and convert them to vectors\nimage_paths = []\nfeatures = []\nfor root, dirs, files in os.walk(\'datasets/\'):\n    for file in files:\n        if file.endswith((".jpg", ".tif", ".png")):\n            file_path = os.path.join(root, file)\n            image_paths.append(file_path)\n\n            img = Image.open(file_path)\n            \n            with torch.no_grad():\n                feature = model.encode_image(preprocess(img).unsqueeze(0).to(device)).detach().cpu().numpy()\n            features.append(feature)\n\n\nprint(len(features), \' Images Found!\')\nf = np.concatenate(features, axis=0)\n'})}),"\n",(0,i.jsx)(n.p,{children:"14099  Images Found!"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Took ~ 2m 30s on NVIDIA 3060"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"dataset-visualization",children:"Dataset Visualization"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Here we randomly select an image from dataset as our query image\nrandompath = image_paths.copy()\nrandom.shuffle(randompath)\n\nn = 50\n\n\n# Plot query image\nrows, columns = 10, 5\nfig, axs = plt.subplots(rows, columns, figsize=(20, 50), squeeze=True)\n\n\nn = 0\nfor i in range(rows):\n    for j in range(columns):\n\n        axs[i][j].imshow(Image.open(randompath[n]))\n        axs[i][j].set_title(f'Instance {n}')\n        n += 1\nplt.show()\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.img,{alt:"png",src:a(344).c+"",width:"800",height:"800"}),"\n",(0,i.jsx)(n.img,{alt:"png",src:a(6244).c+"",width:"800",height:"800"}),"\n",(0,i.jsx)(n.img,{alt:"png",src:a(7424).c+"",width:"800",height:"800"})]}),"\n",(0,i.jsx)(n.h3,{id:"nn-search",children:"NN Search"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Text Embedding (Query feature)\nquery = 'stadium'\nquery = clip.tokenize(query).to(device)\nquery = model.encode_text(query).detach().cpu()\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# NN Search\nk = 50\nneigh = NearestNeighbors(n_neighbors=k, algorithm='brute')\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"neigh.fit(f)\ndistances, indices = neigh.kneighbors(query)\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"rows, columns = 10, 5\nfig, axs = plt.subplots(rows, columns, figsize=(20, 50))\n\n\nn = 0\nfor i in range(rows):\n    for j in range(columns):\n\n        axs[i][j].imshow(Image.open(image_paths[indices[0][n]]))\n        axs[i][j].set_title(f'Nearest Neighbor {n}')\n        n += 1\nplt.show()\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.img,{alt:"png",src:a(3716).c+"",width:"800",height:"800"}),"\n",(0,i.jsx)(n.img,{alt:"png",src:a(9168).c+"",width:"800",height:"800"}),"\n",(0,i.jsx)(n.img,{alt:"png",src:a(9352).c+"",width:"800",height:"800"})]})]})}function h(e={}){const{wrapper:n}={...(0,t.MN)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},5788:(e,n,a)=>{a.d(n,{MN:()=>c});var i=a(1504);function t(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function s(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),a.push.apply(a,i)}return a}function r(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?s(Object(a),!0).forEach((function(n){t(e,n,a[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):s(Object(a)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))}))}return e}function o(e,n){if(null==e)return{};var a,i,t=function(e,n){if(null==e)return{};var a,i,t={},s=Object.keys(e);for(i=0;i<s.length;i++)a=s[i],n.indexOf(a)>=0||(t[a]=e[a]);return t}(e,n);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(i=0;i<s.length;i++)a=s[i],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(t[a]=e[a])}return t}var l=i.createContext({}),c=function(e){var n=i.useContext(l),a=n;return e&&(a="function"==typeof e?e(n):r(r({},n),e)),a},d={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},h=i.forwardRef((function(e,n){var a=e.components,t=e.mdxType,s=e.originalType,l=e.parentName,h=o(e,["components","mdxType","originalType","parentName"]),m=c(a),g=t,p=m["".concat(l,".").concat(g)]||m[g]||d[g]||s;return a?i.createElement(p,r(r({ref:n},h),{},{components:a})):i.createElement(p,r({ref:n},h))}));h.displayName="MDXCreateElement"},344:(e,n,a)=>{a.d(n,{c:()=>i});const i=a.p+"assets/images/dataset-p1-e2d66d066b0a64948364a2a10a9dc25a.png"},6244:(e,n,a)=>{a.d(n,{c:()=>i});const i=a.p+"assets/images/dataset-p2-bcb1d1ea01ca2ac0a56a0ab13bd586a3.png"},7424:(e,n,a)=>{a.d(n,{c:()=>i});const i=a.p+"assets/images/dataset-p3-9430d921b48a8fb5396ff293daf428f4.png"},3604:(e,n,a)=>{a.d(n,{c:()=>i});const i=a.p+"assets/images/header-7343f4b47de766d8ff193e299f6ba3b8.png"},3716:(e,n,a)=>{a.d(n,{c:()=>i});const i=a.p+"assets/images/neighbors-1-10fe4921156ecae2d7fe3624a4b4a57a.png"},9168:(e,n,a)=>{a.d(n,{c:()=>i});const i=a.p+"assets/images/neighbors-2-b98417145899ff7f065caec18062a483.png"},9352:(e,n,a)=>{a.d(n,{c:()=>i});const i=a.p+"assets/images/neighbors-3-f9f141256e3fb36a9c2b7d89cfa89c43.png"}}]);