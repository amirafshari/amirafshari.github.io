"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[1477],{10:t=>{t.exports=JSON.parse('{"blogPosts":[{"id":"license-plate-detector","metadata":{"permalink":"/blog/license-plate-detector","editUrl":"https://github.com/amirafshari/amirafshari.github.io/tree/main/blog/2021-09-02-license-plate-detector.md","source":"@site/blog/2021-09-02-license-plate-detector.md","title":"Automatic License Plate Detector","description":"result-9","date":"2021-09-02T00:00:00.000Z","formattedDate":"September 2, 2021","tags":[{"label":"Object Detection","permalink":"/blog/tags/object-detection"},{"label":"Computer Vision","permalink":"/blog/tags/computer-vision"},{"label":"Deep Learning","permalink":"/blog/tags/deep-learning"}],"readingTime":2.315,"hasTruncateMarker":true,"authors":[{"name":"Amir Afshari","title":"Machine Learning Engineer","url":"https://github.com/amirafshari","image_url":"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4","imageURL":"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4"}],"frontMatter":{"slug":"license-plate-detector","title":"Automatic License Plate Detector","authors":{"name":"Amir Afshari","title":"Machine Learning Engineer","url":"https://github.com/amirafshari","image_url":"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4","imageURL":"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4"},"tags":["Object Detection","Computer Vision","Deep Learning"]},"nextItem":{"title":"Train a Custom Object Detector","permalink":"/blog/train-custom-object-detector"}},"content":"![result-9](https://user-images.githubusercontent.com/17769927/134549834-da73a045-05c9-4d6c-8772-90c4dca67cf7.jpg)\\n\x3c!--truncate--\x3e\\n\\n\\n## Exploratory Data Analysis\\nHow our data looks like?  \\nAnnotations format (YOLO Format): [class, x_center, y_center, obj_width, obj_height]  \\n\\n\\n### Distributions\\n\\n![1](https://user-images.githubusercontent.com/17769927/134396237-178893ef-18f1-4df6-b3ea-fe4b235e3a27.png)\\n     \\n\\n\\nThey make sense for number plate images  \\n*   x values are well distributed, which means the cameraman did a good job :D\\n*   y values are well distributed as well, but, most of the objects are on top of our images.\\n*   both height and width make sense, because our object is licence plate and they all have almost similiar sizes.\\n\\n\\n### X vs Y & Height vs Width\\n\\n![2](https://user-images.githubusercontent.com/17769927/134396293-df5113b7-9237-4dfc-81ac-1a2bf6187826.png)\\n\\n*   As mentioned above, there is a lack in our dataset in buttom-half part of xy plane.\\n*   As we can see, the center of our x axis is dense, it\'s beacuse humans put the object in the center of the camera.\\n\\n\\n\\n## Tensorflow Implementation for YOLOv4\\n**It\'s [recommended](https://github.com/hunglc007/tensorflow-yolov4-tflite#traning-your-own-model) to train your custom detector on [darknet](https://amirafshari.com/blog/train-custom-object-detector), rather than this implemntation, and then convert your weights and use this implemntation.**\\n\\n\\n```python\\n!git clone https://github.com/hunglc007/tensorflow-yolov4-tflite\\n```\\n\\n### Environment Setup\\n\\n#### Conda Environment\\n\\n\\n```python\\n# Create\\n# tf < 2.5 | python = 3.7\\n# tf > 2.5 | python > 3.9\\n!conda create --name envname python=3.7\\n\\n# Activate\\n!activate envname\\n```\\n\\n#### Requirements\\n\\n\\n```python\\n# in tf > 2.5 both cpu and gpu use the same package\\n\\n# GPU\\n!pip install -r requirements-gpu.txt\\n\\n# CPU\\n!pip install -r requirements.txt\\n```\\n\\n#### Check\\n\\n\\n```python\\n!conda list # installed packages in current env\\n!python --version\\n```\\n\\n#### Set the environment as jupyter kernel\\n\\n\\n```python\\n!pip install ipykernel\\n```\\n\\n\\n```python\\n!python -m ipykernel install --user --name=envname\\n```\\n\\nThen choose yolov4tf from kernels in your notebook\\n\\n### Tensorflow\\n\\n#### Convert weights\\n\\n\\n```python\\n!python save_model.py --weights ./data/yolov4.weights --output ./checkpoints/yolov4-416 --input_size 416 --model yolov4\\n```\\n\\n#### COCO Dataset\\n\\n```python\\n!python detect.py --weights ./checkpoints/yolov4-416 --size 416 --model yolov4 --image ./data/kite.jpg\\n```\\n\\n#### Custom Dataset\\n\\n*   Create a custom.names file in data/classes and type your class (based on your weights and training)\\n*   Call the custom.names in config.py (change coco.names to custom.names)\\n*   Change the paths in detect.py\\n    \\n\\n\\n```python\\n!python detect.py --weights ./checkpoints/custom --size 416 --model yolov4 --image ./data/custom.jpg\\n```\\n![result](https://user-images.githubusercontent.com/17769927/134549864-703159d9-a8f2-41d0-b4ef-48e52bf770b9.jpg)\\n\\n### 3. Tflite\\nRecommended for mobile and edge devices.\\n\\n#### Convert\\n\\n```python\\n# Save tf model for tflite converting\\n!python save_model.py --weights ./data/yolov4.weights --output ./checkpoints/yolov4-416 --input_size 416 --model yolov4 --framework tflite\\n\\n# YOLOv4\\n!python convert_tflite.py --weights ./checkpoints/yolov4-416 --output ./checkpoints/yolov4-416.tflite\\n```\\n\\n#### Demo\\n\\n```python\\n!python detect.py --weights ./checkpoints/yolov4-416.tflite --size 416 --model yolov4 --image ./data/kite.jpg --framework tflite\\n```\\n![result-9](https://user-images.githubusercontent.com/17769927/134549834-da73a045-05c9-4d6c-8772-90c4dca67cf7.jpg)\\n\\n\\n## Metrics\\n\\n*   Precision: 91 %\\n*   Average Precision: 89.80 %\\n*   Recall: 86 %\\n*   F1-score: 88 %\\n*   Average IoU: 74.06 %\\n*   mAP@0.5: 89.80 %\\n*   Confusion Matrix:\\n    *   TP = 439\\n    *   FP = 45\\n    *   FN = 73\\n    *   unique_truth_count (TP+FN) = 512\\n    *   detections_count = 805"},{"id":"train-custom-object-detector","metadata":{"permalink":"/blog/train-custom-object-detector","editUrl":"https://github.com/amirafshari/amirafshari.github.io/tree/main/blog/2021-08-28-train-custom-object-detector.md","source":"@site/blog/2021-08-28-train-custom-object-detector.md","title":"Train a Custom Object Detector","description":"download","date":"2021-08-28T00:00:00.000Z","formattedDate":"August 28, 2021","tags":[{"label":"Object Detection","permalink":"/blog/tags/object-detection"},{"label":"Computer Vision","permalink":"/blog/tags/computer-vision"},{"label":"Deep Learning","permalink":"/blog/tags/deep-learning"}],"readingTime":2.22,"hasTruncateMarker":true,"authors":[{"name":"Amir Afshari","title":"Machine Learning Engineer","url":"https://github.com/amirafshari","image_url":"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4","imageURL":"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4"}],"frontMatter":{"slug":"train-custom-object-detector","title":"Train a Custom Object Detector","authors":{"name":"Amir Afshari","title":"Machine Learning Engineer","url":"https://github.com/amirafshari","image_url":"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4","imageURL":"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4"},"tags":["Object Detection","Computer Vision","Deep Learning"]},"prevItem":{"title":"Automatic License Plate Detector","permalink":"/blog/license-plate-detector"},"nextItem":{"title":"Housing Data Analysis","permalink":"/blog/housing-data-analysis"}},"content":"![download](https://user-images.githubusercontent.com/17769927/138662797-827178bd-ce03-4896-b093-1705c3ac6d4f.png)\\n\x3c!--truncate--\x3e\\n\\n\\n## Darknet Configurations\\n**This documentation is for Google Colab. If you want to know how to compile darknet on your linux local machine (Ubuntu 20.04), please read [this documentation](https://github.com/amirafshari/LPD-YOLOv4/blob/master/darknet-linux.md).**\\n\\n```python\\n# clone repo\\n#!git clone https://github.com/AlexeyAB/darknet\\n!git clone https://github.com/amirafshari/LPD-YOLOv4\\n```\\n\\n### GPU\\n```python\\n# change makefile to have GPU and OPENCV enabled\\n%cd darknet\\n!sed -i \'s/OPENCV=0/OPENCV=1/\' Makefile\\n!sed -i \'s/GPU=0/GPU=1/\' Makefile\\n!sed -i \'s/CUDNN=0/CUDNN=1/\' Makefile\\n!sed -i \'s/CUDNN_HALF=0/CUDNN_HALF=1/\' Makefile\\n```\\n\\n```python\\n# verify CUDA\\n!/usr/local/cuda/bin/nvcc --version\\n```\\n\\n```python\\n# make darknet\\n!make\\n```\\n\\n\\n### Weights\\n```python\\n# pre-trained weights on MS COCO dataset\\n!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights\\n```\\n\\n```python\\n# pre-trained weights for the convolutional layers\\n!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137\\n```\\n\\n\\n### Generate train.txt and test.txt\\nThese files are not in the official repo, but you can find them in my repository.\\n\\n```python\\n!python generate_train.py\\n!python generate_test.py\\n```\\n\\n### Configurations\\nWe need to change/create these files (I configured them for our object (which is license plate), and put them in this repository):\\n*   data/obj.names\\n*   data/obj.data\\n*   cfg/yolov4-custom.cgf\\n*   cfg/yolov4-obj.cfg\\n\\n\\n## Training\\n### Configurations\\nhttps://github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects  \\n\\n*   1 Epoch = images_in_train_txt / batch = 2000 / 32 = 62.5\\n\\n\\n\\n### Train\\n\\n```python\\n# Access Denied Error\\n!chmod +x ./darknet\\n```\\n\\n```python\\n# set custom cfg to train mode \\n%cd cfg\\n!sed -i \'s/batch=1/batch=64/\' yolov4-obj.cfg\\n!sed -i \'s/subdivisions=1/subdivisions=16/\' yolov4-obj.cfg\\n%cd ..\\n```\\n\\n```python\\n!./darknet detector train ./data/obj.data ./cfg/yolov4-obj.cfg yolov4.conv.137 -dont_show -map\\n```\\n\\n### Restart\\nIn case of intruption, we can restart training from our last weight.  \\n(every 100 iterations our weights are saved to backup folder in yolov4-obj_last.weights) (~every 30 minutes)  \\n(every 1000 iterations our weight are saved to backup folder in yolo-obj_xxxx.weights)\\n\\n```python\\n!./darknet detector train ./data/obj.data ./cfg/yolov4-obj.cfg ./backup/yolov4-obj_last.weights -dont_show -map\\n```\\n\\n## Sanity Check\\n\\n#### Setup\\n\\n```python\\n# set custom cfg to test mode \\n%cd cfg\\n!sed -i \'s/batch=64/batch=1/\' yolov4-obj.cfg\\n!sed -i \'s/subdivisions=16/subdivisions=1/\' yolov4-obj.cfg\\n%cd ..\\n```\\n\\n```python\\ndef imShow(path):\\n  import cv2\\n  import matplotlib.pyplot as plt\\n  %matplotlib inline\\n\\n  image = cv2.imread(path)\\n  height, width = image.shape[:2]\\n  resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)\\n\\n  fig = plt.gcf()\\n  fig.set_size_inches(18, 10)\\n  plt.axis(\\"off\\")\\n  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\\n  plt.show()\\n```\\n\\n#### COCO Dataset\\n\\n```python\\n!./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights data/person.jpg\\n```\\n\\n```python\\nimShow(\'./predictions.jpg\')\\n```\\n![download](https://user-images.githubusercontent.com/17769927/138662797-827178bd-ce03-4896-b093-1705c3ac6d4f.png)\\n\\n\\n#### Custom Dataset\\n\\n```python\\n!./darknet detector test ./data/obj.data ./cfg/yolov4-obj.cfg ./backup/yolov4-obj_last.weights ../Cars354.png -thresh 0.3\\n```\\n\\n```python\\nimShow(\'./predictions.jpg\')\\n```\\n![result-4](https://user-images.githubusercontent.com/17769927/134551901-37ff3f6d-37ae-42dc-96c3-8064786355fe.jpg)\\n\\n\\n**To process a list of images data/train.txt and save results of detection to result.json file use**\\n\\n```python\\n!./darknet detector test data/obj.data cfg/yolov4-obj.cfg backup/yolov4-obj_last.weights -ext_output -dont_show -out result.json < data/test.txt\\n```\\n\\n## Metrics\\n**Use -map flag while training for charts**  \\nmAP-chart (red-line) and Loss-chart (blue-line) will be saved in root directory.  \\nmAP will be calculated for each 4 Epochs ~ 240 batches\\n\\n```python\\n!./darknet detector map data/obj.data cfg/yolov4-obj.cfg backup/custom.weights\\n```"},{"id":"housing-data-analysis","metadata":{"permalink":"/blog/housing-data-analysis","editUrl":"https://github.com/amirafshari/amirafshari.github.io/tree/main/blog/2020-08-28-housing-data-analysis.md","source":"@site/blog/2020-08-28-housing-data-analysis.md","title":"Housing Data Analysis","description":"density2","date":"2020-08-28T00:00:00.000Z","formattedDate":"August 28, 2020","tags":[{"label":"Data Analysis","permalink":"/blog/tags/data-analysis"},{"label":"Data Visualization","permalink":"/blog/tags/data-visualization"}],"readingTime":0.42,"hasTruncateMarker":true,"authors":[{"name":"Amir Afshari","title":"Machine Learning Engineer","url":"https://github.com/amirafshari","image_url":"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4","imageURL":"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4"}],"frontMatter":{"slug":"housing-data-analysis","title":"Housing Data Analysis","authors":{"name":"Amir Afshari","title":"Machine Learning Engineer","url":"https://github.com/amirafshari","image_url":"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4","imageURL":"https://avatars.githubusercontent.com/u/17769927?s=400&u=d630f608970a53d00295f2e87e88526b41b7d0b1&v=4"},"tags":["Data Analysis","Data Visualization"]},"prevItem":{"title":"Train a Custom Object Detector","permalink":"/blog/train-custom-object-detector"}},"content":"![density2](https://user-images.githubusercontent.com/17769927/138718424-6f6cd0c6-c5a6-4442-a058-80968fc29035.png)\\n\x3c!--truncate--\x3e\\n\\nThis dataset is based on data from the 1990 California census. Our dataset consist of 26k samples with 10 features.\\n\\nfeatures = [longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, median_house_value, ocean_proximity]\\n\\n## Ocean Proximity Count\\nOcean Proximity is our only categorical data in this dataset.\\n\\n![ocean_proximity](https://user-images.githubusercontent.com/17769927/138718362-f419fed8-b34c-41cb-9cb2-09f3ce13e5e5.png)\\n![ocean_proximity-pie](https://user-images.githubusercontent.com/17769927/138718371-c17dede1-247e-4090-b54f-6d650121d9cb.png)\\n\\n## Where are the most populated areas?\\n### Population Density Recongnition\\n![density](https://user-images.githubusercontent.com/17769927/138718446-ca1aab50-45b3-4094-86ef-bcb9a4fb07fc.png)\\n![density1](https://user-images.githubusercontent.com/17769927/138718413-3e29fa00-2f4f-4967-a752-39ce56fb455b.png)\\n![density2](https://user-images.githubusercontent.com/17769927/138718424-6f6cd0c6-c5a6-4442-a058-80968fc29035.png)\\n\\n## Correlations\\n### Median Income vs Median House Value (Strongest Correlation)\\n![correlations](https://user-images.githubusercontent.com/17769927/138719144-34f2ed03-e332-4646-811b-3569a8d5f7ae.png)\\n\\n## Distribution of Features\\n![feature-distribution](https://user-images.githubusercontent.com/17769927/138719163-09cf03d6-64e9-409e-b647-66529e2abb73.png)\\n\\n## Outliers\\n![outliers](https://user-images.githubusercontent.com/17769927/138719182-5addddd8-398f-4cb5-9bd7-b293820d9ff2.png)"}]}')}}]);