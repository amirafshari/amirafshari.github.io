<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://amirafshari.github.io/blog</id>
    <title>Amir Afshari Blog</title>
    <updated>2024-05-18T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://amirafshari.github.io/blog"/>
    <subtitle>Amir Afshari Blog</subtitle>
    <icon>https://amirafshari.github.io/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[Satellite Data Vector Database]]></title>
        <id>https://amirafshari.github.io/blog/remote-sensing-image-search-cbir-becnchmark</id>
        <link href="https://amirafshari.github.io/blog/remote-sensing-image-search-cbir-becnchmark"/>
        <updated>2024-05-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[png]]></summary>
        <content type="html"><![CDATA[<p><img alt="png" src="https://amirafshari.github.io/assets/images/image-search_60_1-640dab6c0bee7b9b33e6a35412c6dd34.png" width="800" height="500"></p>
<p>With the rapid advancement of remote sensing technologies, the availability of large-scale satellite image datasets has grown exponentially. These datasets contain invaluable information for various applications, including environmental monitoring, urban planning, and disaster management. However, extracting specific categories of objects, such as identifying all images which are similar to a sepcific query image
within a dataset of thousands or millions or even billions of samples, presents a significant challenge for human analyst.
ships within a dataset of one million samples, presents a significant challenge due to the sheer volume of data and the complexity of manual analysis.</p>
<h3 id="solution">Solution</h3>
<p>This task, which is overwhelming for human analysts, can be efficiently addressed using vector search techniques. By leveraging deep learning models to transform images into high-dimensional vectors and utilizing various models such as classification, segmentation, etc, we can use their last layer features and employ nearest neighbor search algorithms to quickly and accurately retrieve relevant images based on their content or semantic meaning.</p>
<p>For instance, you find and interesting shape in your dataset and you want to figure out if there is any similar image in your dataset or not? To do so, you can use that image as a search query to find the similar images.</p>
<h3 id="good-to-know">Good to know</h3>
<ul>
<li>I just fine-tuned ResNet50 on AID dataset, we can train more architectures on different datasets and benchmark them.</li>
<li>By fine-tuning on a large-scale dataset, the accuracy significantly improves.</li>
<li>It's not a production-level (or even near-production-level) solution, so there is plenty of room for improvement in both speed and accuracy.</li>
<li>We can use vector databases such as Weaviate or Redis instead of a simple Python list.</li>
<li>It's good to dig into the datasets first and then judge the performance of the model.</li>
<li>You can find the <a href="https://github.com/amirafshari/rs-cbir">code on Github</a></li>
</ul>
<pre><code class="language-python">import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd
import numpy as np 
import torch
import torch.nn as nn
from torchvision import models, transforms, utils
from torch.utils.data import Dataset, DataLoader, random_split
from skimage import io, transform
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import confusion_matrix, accuracy_score

import os
import glob
import random

from PIL import Image
</code></pre>
<pre><code class="language-python">device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device
</code></pre>
<p>device(type='cuda')</p>
<h2 id="datasets">Datasets</h2>
<h3 id="aid">AID</h3>
<p>To Do</p>
<ul>
<li>Dataset Details</li>
</ul>
<pre><code class="language-python">class AID(Dataset):

    def __init__(self, root_dir):

        self.label_map = {}
        self.info = []
        i = 0
        for root, dirs, files in os.walk(root_dir):
                
            for file in files:
                
                if file.endswith((".jpg", ".tif", ".png", "jpeg")):
                    file_path = os.path.join(root, file)
                    label = root.split('/')[-1]
                    self.info.append((file_path, label.lower()))


                if label.lower() not in self.label_map:
                    self.label_map[label.lower()] = i
                    i += 1

        random.shuffle(self.info)
        self.transform = transform


    def map_labels(self, label):
        return self.label_map[label]





    def __len__(self):
        return len(self.info)


    def __getitem__(self, idx):
        img_path, label = self.info[idx]

        label = self.map_labels(label.lower())

        img = io.imread(img_path)[:,:,:3]
        img = img / 255.
        img = torch.tensor(img, dtype=torch.float32)
        
        return img, label, img_path
</code></pre>
<pre><code class="language-python">dataset = AID('datasets/AID')

label_map = dataset.label_map
class_map = {v: k for k, v in label_map.items()}
classes = [v for k,v in class_map.items()]
</code></pre>
<pre><code class="language-python">rows, columns = 2, 5
fig, axs = plt.subplots(rows, columns, figsize=(21, 8), squeeze=True)


n = 0
for i in range(rows):
    for j in range(columns):
        img, label, _ = dataset[n]
        clss = class_map[label]

        axs[i][j].imshow(img)
        axs[i][j].set_title(clss)
        n += 1
plt.show()
</code></pre>
<p><img alt="png" src="https://amirafshari.github.io/assets/images/image-search_8_0-25c11ee343a89a4df15c21a610eb0aff.png" width="800" height="300"></p>
<h3 id="fair1m">FAIR1M</h3>
<pre><code class="language-python">class FAIR1M(Dataset):

    def __init__(self, root_dir, transform=None):

        self.label_map = {
            'ship': 0,
            'airplane': 1,
            'neighborhood': 2,
        }

        self.info = []
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                if file.endswith((".jpg", ".tif", ".png", "jpeg")):
                    file_path = os.path.join(root, file)
                    label = root.split('/')[-1]
                    self.info.append((file_path, label.lower()))

        random.shuffle(self.info)
        self.transform = transform

    def map_labels(self, label):
        return self.label_map[label]





    def __len__(self):
        return len(self.info)


    def __getitem__(self, idx):
        img_path, label = self.info[idx]

        label = self.map_labels(label.lower())

        img = io.imread(img_path)[:,:,:3]
        img = img / 255.
        img = torch.tensor(img, dtype=torch.float32)
        
        return img, label, img_path
</code></pre>
<pre><code class="language-python">dataset = FAIR1M('datasets/FAIR1M_partial')

class_map = {
    0: 'ship',
    1: 'airplane',
    2: 'neighborhood',
}
classes = [v for k,v in class_map.items()]
classes
</code></pre>
<p>['ship', 'airplane', 'neighborhood']</p>
<pre><code class="language-python">rows, columns = 2, 5
fig, axs = plt.subplots(rows, columns, figsize=(21, 8), squeeze=True)


n = 0
for i in range(rows):
    for j in range(columns):
        img, label, _ = dataset[n]
        clss = class_map[label]

        axs[i][j].imshow(img)
        axs[i][j].set_title(clss)
        n += 1
plt.show()
</code></pre>
<p><img alt="png" src="https://amirafshari.github.io/assets/images/image-search_13_0-13d2af7f5f68dc6d50f351728fd8ddf6.png" width="800" height="300"></p>
<h3 id="resisc45">RESISC45</h3>
<pre><code class="language-python">class RESISC45(Dataset):

    def __init__(self, root_dir, transform=None):

        self.label_map = {
            'ship': 0,
            'airplane': 1,
            'bridge': 2,
        }

        self.info = []
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                if file.endswith((".jpg", ".tif", ".png", "jpeg")):
                    file_path = os.path.join(root, file)
                    label = root.split('/')[-1]
                    self.info.append((file_path, label))

        random.shuffle(self.info)
        self.transform = transform

    def map_labels(self, label):
        return self.label_map[label]





    def __len__(self):
        return len(self.info)


    def __getitem__(self, idx):
        img_path, label = self.info[idx]

        label = self.map_labels(label.lower())

        img = io.imread(img_path)[:,:,:3]
        img = img / 255.
        img = torch.tensor(img, dtype=torch.float32)
        
        return img, label, img_path
</code></pre>
<pre><code class="language-python">dataset = RESISC45('datasets/RESISC45_partial')
</code></pre>
<pre><code class="language-python">class_map = {
    0: 'ship',
    1: 'airplane',
    2: 'bridge',
}
classes = [v for k,v in class_map.items()]
classes
</code></pre>
<p>['ship', 'airplane', 'bridge']</p>
<pre><code class="language-python">rows, columns = 2, 5
fig, axs = plt.subplots(rows, columns, figsize=(21, 8), squeeze=True)


n = 0
for i in range(rows):
    for j in range(columns):
        img, label, _ = dataset[n]
        clss = class_map[label]

        axs[i][j].imshow(img)
        axs[i][j].set_title(clss)
        n += 1
plt.show()
</code></pre>
<p><img alt="png" src="https://amirafshari.github.io/assets/images/image-search_18_0-12cbcd9abf2444e3218ae01195aabf52.png" width="800" height="300"></p>
<h3 id="sentinel-2-ship">Sentinel-2 ship</h3>
<pre><code class="language-python">class SS2(Dataset):

    def __init__(self, root_dir, transform=None):

        self.label_map = {
            'ship': 0,
            'noship': 1,
        }

        self.info = []
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                if file.endswith((".jpg", ".tif", ".png", "jpeg")):
                    file_path = os.path.join(root, file)
                    label = root.split('/')[-1]
                    self.info.append((file_path, label))

        random.shuffle(self.info)
        self.transform = transform

    def map_labels(self, label):
        return self.label_map[label]





    def __len__(self):
        return len(self.info)


    def __getitem__(self, idx):
        img_path, label = self.info[idx]

        label = self.map_labels(label.lower())

        img = io.imread(img_path)[:,:,:3]
        img = img / 255.
        img = torch.tensor(img, dtype=torch.float32)
        
        return img, label, img_path
</code></pre>
<pre><code class="language-python">dataset = SS2('datasets/Sentinel2_partial')
</code></pre>
<pre><code class="language-python">class_map = {
    0: 'ship',
    1: 'noship',
}
classes = [v for k,v in class_map.items()]
classes
</code></pre>
<p>['ship', 'noship']</p>
<pre><code class="language-python">rows, columns = 2, 5
fig, axs = plt.subplots(rows, columns, figsize=(21, 8), squeeze=True)


n = 0
for i in range(rows):
    for j in range(columns):
        img, label, _ = dataset[n]
        clss = class_map[label]

        axs[i][j].imshow(img)
        axs[i][j].set_title(clss)
        n += 1
plt.show()
</code></pre>
<p><img alt="png" src="https://amirafshari.github.io/assets/images/image-search_23_0-09e7da946801ed88c33a4bfde78c9efc.png" width="800" height="300"></p>
<h2 id="models">Models</h2>
<ul>
<li><a href="https://pytorch.org/vision/stable/models.html">https://pytorch.org/vision/stable/models.html</a></li>
<li><a href="https://pytorch.org/vision/main/models">https://pytorch.org/vision/main/models</a></li>
<li>load your neural netwrok for feature extraction</li>
</ul>
<p>To Do</p>
<ul>
<li>Use fine-tuned weights</li>
<li>Use different architectures for this task and compare their infernce speed and accuracy.</li>
</ul>
<pre><code class="language-python">weights = ['IMAGENET1K_V1', 'IMAGENET1K_V2', 'IMAGENET1K_SWAG_E2E_V1', 'IMAGENET1K_SWAG_E2E_V1', 'IMAGENET1K_SWAG_LINEAR_V1']
</code></pre>
<p>This is a simple way to load a pre-trained model using PyTorch</p>
<pre><code class="language-python">model = torch.hub.load("pytorch/vision", 'resnet50', weights="IMAGENET1K_V2")
</code></pre>
<h3 id="swin">Swin</h3>
<p>Pre-trained on ImageNet</p>
<pre><code class="language-python"># Swin
from torchvision.models import swin_v2_b, Swin_B_Weights

model = models.swin_v2_b(weights=Swin_B_Weights).to(device).eval()

total_params = '{:,}'.format(sum(p.numel() for p in model.parameters()))
total_params
</code></pre>
<pre><code class="language-python">model
</code></pre>
<h3 id="resnet">ResNet</h3>
<p>Pre-trained on ImageNet</p>
<pre><code class="language-python"># model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2).to(device).eval()
# model = models.resnet50(pretrained=True)
</code></pre>
<p>Fine-tuned on AID</p>
<pre><code class="language-python">model = models.resnet50(pretrained=False)
</code></pre>
<p>/home/amir/miniconda3/envs/faiss/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
warnings.warn(
/home/amir/miniconda3/envs/faiss/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or <code>None</code> for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing <code>weights=None</code>.
warnings.warn(msg)</p>
<pre><code class="language-python">model
</code></pre>
<p>Modify the last layer based on your fine-tuned weights</p>
<pre><code class="language-python">num_classes = 17 # AID
# num_classes = 43 # BigEarthNet

model.fc = torch.nn.Linear(model.fc.in_features, num_classes)
</code></pre>
<pre><code class="language-python">checkpoint = torch.load('weights/aid_multilabel_scratch_resnet50.pth', map_location=torch.device('cpu'))
state_dict = checkpoint['state_dict']
model.load_state_dict(state_dict)
model.to(device).eval()
</code></pre>
<pre><code>    ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=2048, out_features=17, bias=True)
    )
</code></pre>
<p>Using this module, we can extract features from the last layer of our network, to use it as our feature extractor for our vector database</p>
<pre><code class="language-python">class ResNetFeatures(torch.nn.Module):
    def __init__(self, original_model):
        super(ResNetFeatures, self).__init__()
        self.features = torch.nn.Sequential(*list(original_model.children())[:-1])  # all layers except the final FC layer
    
    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, 1)
        return x

# Create an instance of the new model
model = ResNetFeatures(model).to(device).eval()
</code></pre>
<pre><code class="language-python">for key in state_dict.keys():
    print(key)
</code></pre>
<p>Number of model parameters</p>
<pre><code class="language-python">total_params = '{:,}'.format(sum(p.numel() for p in model.parameters()))
total_params
</code></pre>
<p>'23,508,032'</p>
<h3 id="regnet">RegNet</h3>
<pre><code class="language-python"># RegNet

model = models.regnet_y_400mf(pretrained=True).to(device).eval()

total_params = '{:,}'.format(sum(p.numel() for p in model.parameters()))
total_params
</code></pre>
<pre><code class="language-python">model
</code></pre>
<h2 id="metrics">Metrics</h2>
<p>To Do</p>
<ul>
<li>What metrics for CBIR?</li>
</ul>
<pre><code class="language-python">def accuracy(query_label, neighbors, labels):
    ''' A simple function to calculate the accuracy for 1 sample @ K'''
    t, f = 0, 0
    for i in neighbors:
        if query_label == labels[i]:
            t += 1
        else:
            f += 1
    return '{:.1%}'.format(t / (t + f))
</code></pre>
<h2 id="vector-database">Vector Database</h2>
<p>Here we use our feature extractor to convert all of our images to a (1, 2048) vector and store them in our vector database (a python list in here) and perform our NN search.</p>
<p>Of course it's not implemented for production level use, but we can scale it using Vector Database such as Weaviate or Pinecone, etc. We can also use simpler implementation with FAISS library for our vector database and NN search.</p>
<pre><code class="language-python">features = []
image_paths = []
labels = []


# Change the first line according to the dataset you want to use
for x, y, img_path in dataset:
    img = x.permute(2, 0, 1).unsqueeze(0)
    
    with torch.no_grad():
        feature = model(img.to(device)).detach().cpu().numpy()

    features.append(feature)
    image_paths.append(img_path)
    labels.append(y)


features = np.concatenate(features, axis=0)
</code></pre>
<pre><code class="language-python">features.shape
</code></pre>
<p>(10000, 2048)</p>
<p><strong>Swin</strong></p>
<ul>
<li>took</li>
</ul>
<p><strong>ResNet</strong></p>
<ul>
<li>Took 4m 20s for AID on NVIDIA 3060</li>
</ul>
<h2 id="nn-search">NN Search</h2>
<pre><code class="language-python">id = random.randint(0, len(dataset))
img, label, img_path = dataset[id]
img = img.permute(2, 0, 1).unsqueeze(0)

query_image_path = img_path
query_label = label

with torch.no_grad():
    query_feature = model(img.to(device)).detach().cpu().numpy()
</code></pre>
<pre><code class="language-python">k = 20

neigh = NearestNeighbors(n_neighbors=k, algorithm='brute')
neigh.fit(features)
distances, indices = neigh.kneighbors(query_feature)
</code></pre>
<pre><code class="language-python"># Plot query image
query_image = Image.open(query_image_path)


print(f'Accuracy for this sample @ k = {k}: {accuracy(query_label, indices[0], labels)}')

rows, columns = 4, 5
fig, axs = plt.subplots(rows, columns, figsize=(34, 21))

n = 0
for i in range(rows):
    for j in range(columns):
        if (i == 0) and (j == 0):
            axs[0][0].imshow(query_image)
            axs[0][0].set_title(f'Query: {class_map[query_label]}')
        else:
            axs[i][j].imshow(Image.open(image_paths[indices[0][n+1]]))
            axs[i][j].set_title(f'NN {n+1}: {class_map[labels[indices[0][n+1]]]}')
            n += 1
</code></pre>
<p>Accuracy for this sample @ k = 20: 100.0%</p>
<p><img alt="png" src="https://amirafshari.github.io/assets/images/image-search_60_1-640dab6c0bee7b9b33e6a35412c6dd34.png" width="800" height="500"></p>
<h2 id="evaluation">Evaluation</h2>
<p>We used the AID dataset for both training and inference</p>
<pre><code class="language-python">def compute_confusion_matrix_and_accuracy(dataset, model, device, features, labels, ks):
    # Initialize a dictionary to hold confusion matrices and accuracies for different k
    results = {k: {'confusion_matrix': None, 'accuracy': None, 'per_class_accuracy': None, 'confusion_matrix_accuracy': None} for k in ks}
    
    # Initialize the NearestNeighbors model
    neigh = NearestNeighbors(n_neighbors=max(ks), algorithm='brute')
    neigh.fit(features)
    
    # Prepare ground truth and predicted labels for confusion matrix calculation
    all_true_labels = []
    all_pred_labels = {k: [] for k in ks}
    
    for i in range(len(dataset)):
        img, label, _ = dataset[i]
        img = img.permute(2, 0, 1).unsqueeze(0)

        with torch.no_grad():
            query_feature = model(img.to(device)).detach().cpu().numpy()

        distances, indices = neigh.kneighbors(query_feature)
        
        true_label = labels[i]
        all_true_labels.append(true_label)
        
        for k in ks:
            neighbors = indices[0][:k]
            predicted_label = np.bincount([labels[n] for n in neighbors]).argmax()
            all_pred_labels[k].append(predicted_label)
    
    # Calculate confusion matrices and accuracies for each k
    for k in ks:
        cm = confusion_matrix(all_true_labels, all_pred_labels[k])
        acc = accuracy_score(all_true_labels, all_pred_labels[k])

        results[k]['confusion_matrix'] = cm
        results[k]['accuracy'] = acc
        # results[k]['per_class_accuracy'] = cm.diagonal() / cm.sum(axis=0)

        conf_mat_acc = np.zeros_like(cm, dtype=float)
        # results[k]['confusion_matrix'].dtype = float
        for i, row in enumerate(cm):
            conf_mat_acc[i] = row / row.sum()
        results[k]['confusion_matrix_accuracy'] = conf_mat_acc
    

    
    return results
</code></pre>
<pre><code class="language-python">def visualize_confusion_matrix(cm, class_names, k):
    plt.figure(figsize=(18, 18))  # Increase figure size
    sns.set(font_scale=1.2)  # Increase font scale
    sns.heatmap(cm, annot=True, fmt='', cmap='Blues', xticklabels=class_names, yticklabels=class_names, annot_kws={"size": 12, "ha": 'center', "va": 'center'}, linewidths=.5, linecolor='black')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title(f'Predictions for k={k}')
    plt.show()
</code></pre>
<pre><code class="language-python">ks = [10, 50, 100]
results = compute_confusion_matrix_and_accuracy(dataset, model, device, features, labels, ks)
</code></pre>
<h3 id="accuracies-confusion-matrix">Accuracies Confusion Matrix</h3>
<pre><code class="language-python">for k in ks:
    visualize_confusion_matrix(np.round(results[k]['confusion_matrix_accuracy'], 2), classes, k)
</code></pre>
<p><img alt="png" src="https://amirafshari.github.io/assets/images/image-search_66_0-63ce4112af9273a7a683d2b1bd74290c.png" width="800" height="800"></p>
<p><img alt="png" src="https://amirafshari.github.io/assets/images/image-search_66_1-295bc34f26651e527ead3f3151ee24e8.png" width="800" height="800"></p>
<p><img alt="png" src="https://amirafshari.github.io/assets/images/image-search_66_2-f3005aba9d4fc86cbf7137f7bf44efc3.png" width="800" height="800"></p>
<h3 id="predictions-confusion-matrix">Predictions Confusion Matrix</h3>
<pre><code class="language-python">for k in ks:
    visualize_confusion_matrix(results[k]['confusion_matrix'], classes, k)
</code></pre>
<p><img alt="png" src="https://amirafshari.github.io/assets/images/image-search_68_0-e6821aa1e926207e49eb3f59f0410da2.png" width="800" height="800"></p>
<p><img alt="png" src="https://amirafshari.github.io/assets/images/image-search_68_1-5c43d7105b41e4da8500c2632af29516.png" width="800" height="800"></p>
<p><img alt="png" src="https://amirafshari.github.io/assets/images/image-search_68_2-61146bbdcc9dfa033d6d0abb7213cbaf.png" width="800" height="800"></p>]]></content>
        <author>
            <name>Amir Afshari</name>
            <uri>https://github.com/amirafshari</uri>
        </author>
        <category label="Deep Learning" term="Deep Learning"/>
        <category label="Machine Learning" term="Machine Learning"/>
        <category label="Computer Vision" term="Computer Vision"/>
        <category label="Remote Sensing" term="Remote Sensing"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Vision-Language Search on Satellite Images]]></title>
        <id>https://amirafshari.github.io/blog/remote-sensing-image-search-cbir</id>
        <link href="https://amirafshari.github.io/blog/remote-sensing-image-search-cbir"/>
        <updated>2024-05-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[png]]></summary>
        <content type="html"><![CDATA[<p><img alt="png" src="https://amirafshari.github.io/assets/images/header-7343f4b47de766d8ff193e299f6ba3b8.png" width="800" height="800"></p>
<p>With the rapid advancement of remote sensing technologies, the availability of large-scale satellite image datasets has grown exponentially. These datasets contain invaluable information for various applications, including environmental monitoring, urban planning, and disaster management. However, extracting specific categories of objects, such as identifying all ships within a dataset of one million samples, presents a significant challenge due to the sheer volume of data and the complexity of manual analysis.</p>
<h3 id="solution">Solution</h3>
<p>This task, which is overwhelming for human analysts, can be efficiently addressed using vector search techniques. By leveraging deep learning models to transform images into high-dimensional vectors and utilizing multimodal vision-text models such as CLIP, we can employ nearest neighbor search algorithms to quickly and accurately retrieve relevant images based on their content. For instance, you can search for "red ship floating on the sea" and use it as the query and the system provides you the appropriate instances of the dataset while there is no metadata available. This approach not only enhances the efficiency of data processing but also significantly improves speed of finding specific categories within vast datasets.</p>
<h3 id="good-to-know">Good to know</h3>
<ul>
<li>I did not fine-tune CLIP for remote sensing text-image pairs, but it still works fine.</li>
<li>By fine-tuning, we can enhance the accuracy.</li>
<li>It's not a production-level (or even near-production-level) solution, so there is plenty of room for improvement in both speed and accuracy.</li>
<li>We can use vector databases such as Weaviate or Redis instead of a simple Python list.</li>
<li>The dataset is limited to approximately 14,000 samples (a combination of AID, FAIR1M_partial, RESICS_partial, ...).</li>
<li>We can binarize the vectors to improve speed.</li>
<li>You can find the <a href="https://github.com/amirafshari/rs-cbir">code on Github</a></li>
</ul>
<pre><code class="language-python">import matplotlib.pyplot as plt
import numpy as np

import torch
from torchvision import models, transforms
from sklearn.neighbors import NearestNeighbors
import clip


import random
import os
import glob

from PIL import Image
</code></pre>
<pre><code class="language-python">device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device
</code></pre>
<p>device(type='cuda')</p>
<h3 id="model">Model</h3>
<ul>
<li><a href="https://github.com/mlfoundations/open_clip">https://github.com/mlfoundations/open_clip</a></li>
<li><a href="https://github.com/openai/CLIP/blob/main/README.md">https://github.com/openai/CLIP/blob/main/README.md</a></li>
<li>CLIP</li>
</ul>
<pre><code class="language-python">model, preprocess = clip.load("ViT-B/32", device=device)
</code></pre>
<h3 id="image-embedding">Image Embedding</h3>
<pre><code class="language-python"># Walk into directoies to find images and convert them to vectors
image_paths = []
features = []
for root, dirs, files in os.walk('datasets/'):
    for file in files:
        if file.endswith((".jpg", ".tif", ".png")):
            file_path = os.path.join(root, file)
            image_paths.append(file_path)

            img = Image.open(file_path)
            
            with torch.no_grad():
                feature = model.encode_image(preprocess(img).unsqueeze(0).to(device)).detach().cpu().numpy()
            features.append(feature)


print(len(features), ' Images Found!')
f = np.concatenate(features, axis=0)
</code></pre>
<p>14099  Images Found!</p>
<ul>
<li>Took ~ 2m 30s on NVIDIA 3060</li>
</ul>
<h3 id="dataset-visualization">Dataset Visualization</h3>
<pre><code class="language-python"># Here we randomly select an image from dataset as our query image
randompath = image_paths.copy()
random.shuffle(randompath)

n = 50


# Plot query image
rows, columns = 10, 5
fig, axs = plt.subplots(rows, columns, figsize=(20, 50), squeeze=True)


n = 0
for i in range(rows):
    for j in range(columns):

        axs[i][j].imshow(Image.open(randompath[n]))
        axs[i][j].set_title(f'Instance {n}')
        n += 1
plt.show()
</code></pre>
<p><img alt="png" src="https://amirafshari.github.io/assets/images/dataset-p1-e2d66d066b0a64948364a2a10a9dc25a.png" width="800" height="800">
<img alt="png" src="https://amirafshari.github.io/assets/images/dataset-p2-bcb1d1ea01ca2ac0a56a0ab13bd586a3.png" width="800" height="800">
<img alt="png" src="https://amirafshari.github.io/assets/images/dataset-p3-9430d921b48a8fb5396ff293daf428f4.png" width="800" height="800"></p>
<h3 id="nn-search">NN Search</h3>
<pre><code class="language-python"># Text Embedding (Query feature)
query = 'stadium'
query = clip.tokenize(query).to(device)
query = model.encode_text(query).detach().cpu()
</code></pre>
<pre><code class="language-python"># NN Search
k = 50
neigh = NearestNeighbors(n_neighbors=k, algorithm='brute')
</code></pre>
<pre><code class="language-python">neigh.fit(f)
distances, indices = neigh.kneighbors(query)
</code></pre>
<pre><code class="language-python">rows, columns = 10, 5
fig, axs = plt.subplots(rows, columns, figsize=(20, 50))


n = 0
for i in range(rows):
    for j in range(columns):

        axs[i][j].imshow(Image.open(image_paths[indices[0][n]]))
        axs[i][j].set_title(f'Nearest Neighbor {n}')
        n += 1
plt.show()
</code></pre>
<p><img alt="png" src="https://amirafshari.github.io/assets/images/neighbors-1-10fe4921156ecae2d7fe3624a4b4a57a.png" width="800" height="800">
<img alt="png" src="https://amirafshari.github.io/assets/images/neighbors-2-b98417145899ff7f065caec18062a483.png" width="800" height="800">
<img alt="png" src="https://amirafshari.github.io/assets/images/neighbors-3-f9f141256e3fb36a9c2b7d89cfa89c43.png" width="800" height="800"></p>]]></content>
        <author>
            <name>Amir Afshari</name>
            <uri>https://github.com/amirafshari</uri>
        </author>
        <category label="Deep Learning" term="Deep Learning"/>
        <category label="Machine Learning" term="Machine Learning"/>
        <category label="Computer Vision" term="Computer Vision"/>
        <category label="Remote Sensing" term="Remote Sensing"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic License Plate Detector]]></title>
        <id>https://amirafshari.github.io/blog/license-plate-detector</id>
        <link href="https://amirafshari.github.io/blog/license-plate-detector"/>
        <updated>2021-09-02T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[result-9]]></summary>
        <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/17769927/134549834-da73a045-05c9-4d6c-8772-90c4dca67cf7.jpg" alt="result-9"></p>
<h2 id="exploratory-data-analysis">Exploratory Data Analysis</h2>
<p>How our data looks like?<br>
<!-- -->Annotations format (YOLO Format): [class, x_center, y_center, obj_width, obj_height]</p>
<h3 id="distributions">Distributions</h3>
<p><img src="https://user-images.githubusercontent.com/17769927/134396237-178893ef-18f1-4df6-b3ea-fe4b235e3a27.png" alt="1"></p>
<p>They make sense for number plate images</p>
<ul>
<li>x values are well distributed, which means the cameraman did a good job <!-- -->:D</li>
<li>y values are well distributed as well, but, most of the objects are on top of our images.</li>
<li>both height and width make sense, because our object is licence plate and they all have almost similiar sizes.</li>
</ul>
<h3 id="x-vs-y--height-vs-width">X vs Y &amp; Height vs Width</h3>
<p><img src="https://user-images.githubusercontent.com/17769927/134396293-df5113b7-9237-4dfc-81ac-1a2bf6187826.png" alt="2"></p>
<ul>
<li>As mentioned above, there is a lack in our dataset in buttom-half part of xy plane.</li>
<li>As we can see, the center of our x axis is dense, it's beacuse humans put the object in the center of the camera.</li>
</ul>
<h2 id="tensorflow-implementation-for-yolov4">Tensorflow Implementation for YOLOv4</h2>
<p><strong>It's <a href="https://github.com/hunglc007/tensorflow-yolov4-tflite#traning-your-own-model">recommended</a> to train your custom detector on <a href="https://amirafshari.com/blog/train-custom-object-detector">darknet</a>, rather than this implemntation, and then convert your weights and use this implemntation.</strong></p>
<pre><code class="language-python">!git clone https://github.com/hunglc007/tensorflow-yolov4-tflite
</code></pre>
<h3 id="environment-setup">Environment Setup</h3>
<h4 id="conda-environment">Conda Environment</h4>
<pre><code class="language-python"># Create
# tf &lt; 2.5 | python = 3.7
# tf &gt; 2.5 | python &gt; 3.9
!conda create --name envname python=3.7

# Activate
!activate envname
</code></pre>
<h4 id="requirements">Requirements</h4>
<pre><code class="language-python"># in tf &gt; 2.5 both cpu and gpu use the same package

# GPU
!pip install -r requirements-gpu.txt

# CPU
!pip install -r requirements.txt
</code></pre>
<h4 id="check">Check</h4>
<pre><code class="language-python">!conda list # installed packages in current env
!python --version
</code></pre>
<h4 id="set-the-environment-as-jupyter-kernel">Set the environment as jupyter kernel</h4>
<pre><code class="language-python">!pip install ipykernel
</code></pre>
<pre><code class="language-python">!python -m ipykernel install --user --name=envname
</code></pre>
<p>Then choose yolov4tf from kernels in your notebook</p>
<h3 id="tensorflow">Tensorflow</h3>
<h4 id="convert-weights">Convert weights</h4>
<pre><code class="language-python">!python save_model.py --weights ./data/yolov4.weights --output ./checkpoints/yolov4-416 --input_size 416 --model yolov4
</code></pre>
<h4 id="coco-dataset">COCO Dataset</h4>
<pre><code class="language-python">!python detect.py --weights ./checkpoints/yolov4-416 --size 416 --model yolov4 --image ./data/kite.jpg
</code></pre>
<h4 id="custom-dataset">Custom Dataset</h4>
<ul>
<li>Create a custom.names file in data/classes and type your class (based on your weights and training)</li>
<li>Call the custom.names in config.py (change coco.names to custom.names)</li>
<li>Change the paths in detect.py</li>
</ul>
<pre><code class="language-python">!python detect.py --weights ./checkpoints/custom --size 416 --model yolov4 --image ./data/custom.jpg
</code></pre>
<p><img src="https://user-images.githubusercontent.com/17769927/134549864-703159d9-a8f2-41d0-b4ef-48e52bf770b9.jpg" alt="result"></p>
<h3 id="3-tflite">3. Tflite</h3>
<p>Recommended for mobile and edge devices.</p>
<h4 id="convert">Convert</h4>
<pre><code class="language-python"># Save tf model for tflite converting
!python save_model.py --weights ./data/yolov4.weights --output ./checkpoints/yolov4-416 --input_size 416 --model yolov4 --framework tflite

# YOLOv4
!python convert_tflite.py --weights ./checkpoints/yolov4-416 --output ./checkpoints/yolov4-416.tflite
</code></pre>
<h4 id="demo">Demo</h4>
<pre><code class="language-python">!python detect.py --weights ./checkpoints/yolov4-416.tflite --size 416 --model yolov4 --image ./data/kite.jpg --framework tflite
</code></pre>
<p><img src="https://user-images.githubusercontent.com/17769927/134549834-da73a045-05c9-4d6c-8772-90c4dca67cf7.jpg" alt="result-9"></p>
<h2 id="metrics">Metrics</h2>
<ul>
<li>Precision: 91 %</li>
<li>Average Precision: 89.80 %</li>
<li>Recall: 86 %</li>
<li>F1-score: 88 %</li>
<li>Average IoU: 74.06 %</li>
<li>mAP@0.5: 89.80 %</li>
<li>Confusion Matrix:<!-- -->
<ul>
<li>TP = 439</li>
<li>FP = 45</li>
<li>FN = 73</li>
<li>unique_truth_count (TP+FN) = 512</li>
<li>detections_count = 805</li>
</ul>
</li>
</ul>]]></content>
        <author>
            <name>Amir Afshari</name>
            <uri>https://github.com/amirafshari</uri>
        </author>
        <category label="Object Detection" term="Object Detection"/>
        <category label="Computer Vision" term="Computer Vision"/>
        <category label="Deep Learning" term="Deep Learning"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Train a Custom Object Detector]]></title>
        <id>https://amirafshari.github.io/blog/train-custom-object-detector</id>
        <link href="https://amirafshari.github.io/blog/train-custom-object-detector"/>
        <updated>2021-08-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[download]]></summary>
        <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/17769927/138662797-827178bd-ce03-4896-b093-1705c3ac6d4f.png" alt="download"></p>
<h2 id="darknet-configurations">Darknet Configurations</h2>
<p><strong>This documentation is for Google Colab. If you want to know how to compile darknet on your linux local machine (Ubuntu 20.04), please read <a href="https://github.com/amirafshari/LPD-YOLOv4/blob/master/darknet-linux.md">this documentation</a>.</strong></p>
<pre><code class="language-python"># clone repo
#!git clone https://github.com/AlexeyAB/darknet
!git clone https://github.com/amirafshari/LPD-YOLOv4
</code></pre>
<h3 id="gpu">GPU</h3>
<pre><code class="language-python"># change makefile to have GPU and OPENCV enabled
%cd darknet
!sed -i 's/OPENCV=0/OPENCV=1/' Makefile
!sed -i 's/GPU=0/GPU=1/' Makefile
!sed -i 's/CUDNN=0/CUDNN=1/' Makefile
!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile
</code></pre>
<pre><code class="language-python"># verify CUDA
!/usr/local/cuda/bin/nvcc --version
</code></pre>
<pre><code class="language-python"># make darknet
!make
</code></pre>
<h3 id="weights">Weights</h3>
<pre><code class="language-python"># pre-trained weights on MS COCO dataset
!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights
</code></pre>
<pre><code class="language-python"># pre-trained weights for the convolutional layers
!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.conv.137
</code></pre>
<h3 id="generate-traintxt-and-testtxt">Generate train.txt and test.txt</h3>
<p>These files are not in the official repo, but you can find them in my repository.</p>
<pre><code class="language-python">!python generate_train.py
!python generate_test.py
</code></pre>
<h3 id="configurations">Configurations</h3>
<p>We need to change/create these files (I configured them for our object (which is license plate), and put them in this repository):</p>
<ul>
<li>data/obj.names</li>
<li>data/obj.data</li>
<li>cfg/yolov4-custom.cgf</li>
<li>cfg/yolov4-obj.cfg</li>
</ul>
<h2 id="training">Training</h2>
<h3 id="configurations-1">Configurations</h3>
<p><a href="https://github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects">https://github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects</a></p>
<ul>
<li>1 Epoch = images_in_train_txt / batch = 2000 / 32 = 62.5</li>
</ul>
<h3 id="train">Train</h3>
<pre><code class="language-python"># Access Denied Error
!chmod +x ./darknet
</code></pre>
<pre><code class="language-python"># set custom cfg to train mode 
%cd cfg
!sed -i 's/batch=1/batch=64/' yolov4-obj.cfg
!sed -i 's/subdivisions=1/subdivisions=16/' yolov4-obj.cfg
%cd ..
</code></pre>
<pre><code class="language-python">!./darknet detector train ./data/obj.data ./cfg/yolov4-obj.cfg yolov4.conv.137 -dont_show -map
</code></pre>
<h3 id="restart">Restart</h3>
<p>In case of intruption, we can restart training from our last weight.<br>
<!-- -->(every 100 iterations our weights are saved to backup folder in yolov4-obj_last.weights) (~every 30 minutes)<br>
<!-- -->(every 1000 iterations our weight are saved to backup folder in yolo-obj_xxxx.weights)</p>
<pre><code class="language-python">!./darknet detector train ./data/obj.data ./cfg/yolov4-obj.cfg ./backup/yolov4-obj_last.weights -dont_show -map
</code></pre>
<h2 id="sanity-check">Sanity Check</h2>
<h4 id="setup">Setup</h4>
<pre><code class="language-python"># set custom cfg to test mode 
%cd cfg
!sed -i 's/batch=64/batch=1/' yolov4-obj.cfg
!sed -i 's/subdivisions=16/subdivisions=1/' yolov4-obj.cfg
%cd ..
</code></pre>
<pre><code class="language-python">def imShow(path):
  import cv2
  import matplotlib.pyplot as plt
  %matplotlib inline

  image = cv2.imread(path)
  height, width = image.shape[:2]
  resized_image = cv2.resize(image,(3*width, 3*height), interpolation = cv2.INTER_CUBIC)

  fig = plt.gcf()
  fig.set_size_inches(18, 10)
  plt.axis("off")
  plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))
  plt.show()
</code></pre>
<h4 id="coco-dataset">COCO Dataset</h4>
<pre><code class="language-python">!./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights data/person.jpg
</code></pre>
<pre><code class="language-python">imShow('./predictions.jpg')
</code></pre>
<p><img src="https://user-images.githubusercontent.com/17769927/138662797-827178bd-ce03-4896-b093-1705c3ac6d4f.png" alt="download"></p>
<h4 id="custom-dataset">Custom Dataset</h4>
<pre><code class="language-python">!./darknet detector test ./data/obj.data ./cfg/yolov4-obj.cfg ./backup/yolov4-obj_last.weights ../Cars354.png -thresh 0.3
</code></pre>
<pre><code class="language-python">imShow('./predictions.jpg')
</code></pre>
<p><img src="https://user-images.githubusercontent.com/17769927/134551901-37ff3f6d-37ae-42dc-96c3-8064786355fe.jpg" alt="result-4"></p>
<p><strong>To process a list of images data/train.txt and save results of detection to result.json file use</strong></p>
<pre><code class="language-python">!./darknet detector test data/obj.data cfg/yolov4-obj.cfg backup/yolov4-obj_last.weights -ext_output -dont_show -out result.json &lt; data/test.txt
</code></pre>
<h2 id="metrics">Metrics</h2>
<p><strong>Use -map flag while training for charts</strong><br>
<!-- -->mAP-chart (red-line) and Loss-chart (blue-line) will be saved in root directory.<br>
<!-- -->mAP will be calculated for each 4 Epochs ~ 240 batches</p>
<pre><code class="language-python">!./darknet detector map data/obj.data cfg/yolov4-obj.cfg backup/custom.weights
</code></pre>]]></content>
        <author>
            <name>Amir Afshari</name>
            <uri>https://github.com/amirafshari</uri>
        </author>
        <category label="Object Detection" term="Object Detection"/>
        <category label="Computer Vision" term="Computer Vision"/>
        <category label="Deep Learning" term="Deep Learning"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Persian Podcasts on Spotify]]></title>
        <id>https://amirafshari.github.io/blog/spotify-persian-podcasts</id>
        <link href="https://amirafshari.github.io/blog/spotify-persian-podcasts"/>
        <updated>2021-06-25T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[spotify-persian-podcast]]></summary>
        <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/17769927/223347442-2b68090c-1d6a-48f5-9cde-b734a87ce5db.png" alt="spotify-persian-podcast"></p>
<pre><code class="language-python"># Recieving Data from Spotify
import spotipy
from spotipy.oauth2 import SpotifyClientCredentials
import pandas as pd
import matplotlib.pyplot as plt
</code></pre>
<h1 id="api">API</h1>
<pre><code class="language-python">sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id="",
                                                           client_secret=""))
</code></pre>
<h1 id="explore">Explore</h1>
<pre><code class="language-python">show = sp.search(q='', type='show', market='US', offset=0, limit=50)
#show['shows']['items'][0]
</code></pre>
<h1 id="prepare">Prepare</h1>
<pre><code class="language-python">names = []
description = []
publisher = []
total_episodes = []
external_urls = []
for i in range(20):
    show = sp.search(q='', type='show', market='US', offset=i*50, limit=50)
    show = show['shows']['items']
    for e in show:
        names.append(e['name'])
        description.append(e['description'])
        publisher.append(e['publisher'])
        total_episodes.append(e['total_episodes'])
        external_urls.append(e['external_urls']['spotify'])
</code></pre>
<pre><code class="language-python">names = pd.Series(names)
description = pd.Series(description)
publisher = pd.Series(publisher)
total_episodes = pd.Series(total_episodes)
external_urls = pd.Series(external_urls)
</code></pre>
<pre><code class="language-python">df = pd.DataFrame(
{
    'Name': names,
    'Publisher': publisher,
    'Description': description,
    'Total Episodes': total_episodes,
    'URL': external_urls,
    
})

df.index += 1
</code></pre>
<h1 id="finally">Finally!</h1>
<pre><code class="language-python">df
</code></pre>
<div><table border="1" class="dataframe"><thead><tr><th></th><th>Name</th><th>Publisher</th><th>Description</th><th>Total Episodes</th><th>URL</th></tr></thead><tbody><tr><th>1</th><td>ChannelB  </td><td>Ali Bandari</td><td>        ...</td><td>83</td><td><a href="https://open.spotify.com/show/2PmMxFZ4OIW5DoUY">https://open.spotify.com/show/2PmMxFZ4OIW5DoUY</a>...</td></tr><tr><th>2</th><td>Masty o Rasty |     </td><td>King Raam</td><td>Welcome to Masty o Rasty "The Drunken Truth" p...</td><td>142</td><td><a href="https://open.spotify.com/show/35RtCrgybsUG3dos">https://open.spotify.com/show/35RtCrgybsUG3dos</a>...</td></tr><tr><th>3</th><td> </td><td>Rokh Podcast</td><td>         </td><td>22</td><td><a href="https://open.spotify.com/show/0hDXe6EN56UZsPBr">https://open.spotify.com/show/0hDXe6EN56UZsPBr</a>...</td></tr><tr><th>4</th><td>Radio Deev /   </td><td>RadioDeev</td><td>        ...</td><td>32</td><td><a href="https://open.spotify.com/show/1KZXcjPkHVeaziGY">https://open.spotify.com/show/1KZXcjPkHVeaziGY</a>...</td></tr><tr><th>5</th><td>   - KetabCast</td><td>  - KetabCast</td><td>  - Ketab Cast      ...</td><td>534</td><td><a href="https://open.spotify.com/show/1eoeo5t8CfjoucLo">https://open.spotify.com/show/1eoeo5t8CfjoucLo</a>...</td></tr><tr><th>...</th><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><th>996</th><td>  | Sagmast</td><td>matinus</td><td>sagmast by Matinus alkohol lovers   ...</td><td>11</td><td><a href="https://open.spotify.com/show/3nEp3Tpo8QLnhVZs">https://open.spotify.com/show/3nEp3Tpo8QLnhVZs</a>...</td></tr><tr><th>997</th><td>  </td><td>Farzad Bayan</td><td>  </td><td>1</td><td><a href="https://open.spotify.com/show/5WRbEroh1Wzb5bvW">https://open.spotify.com/show/5WRbEroh1Wzb5bvW</a>...</td></tr><tr><th>998</th><td>NightPods -  </td><td>NoN Residential</td><td>         ...</td><td>5</td><td><a href="https://open.spotify.com/show/1o2Kvl0Q0rrOCVEZ">https://open.spotify.com/show/1o2Kvl0Q0rrOCVEZ</a>...</td></tr><tr><th>999</th><td>:     </td><td>1343</td><td>        ...</td><td>2</td><td><a href="https://open.spotify.com/show/5ZAb7t3FF1A7eKrD">https://open.spotify.com/show/5ZAb7t3FF1A7eKrD</a>...</td></tr><tr><th>1000</th><td> </td><td>Hesam Bokazadeh</td><td>         ...</td><td>9</td><td><a href="https://open.spotify.com/show/0IOjqr6qtaaFw9B4">https://open.spotify.com/show/0IOjqr6qtaaFw9B4</a>...</td></tr></tbody></table><p>1000 rows  5 columns</p></div>
<h2 id="top-20">Top 20</h2>
<pre><code class="language-python">df[:20]
</code></pre>
<div><table border="1" class="dataframe"><thead><tr><th></th><th>Name</th><th>Publisher</th><th>Description</th><th>Total Episodes</th><th>URL</th></tr></thead><tbody><tr><th>1</th><td>ChannelB  </td><td>Ali Bandari</td><td>        ...</td><td>83</td><td><a href="https://open.spotify.com/show/2PmMxFZ4OIW5DoUY">https://open.spotify.com/show/2PmMxFZ4OIW5DoUY</a>...</td></tr><tr><th>2</th><td>Masty o Rasty |     </td><td>King Raam</td><td>Welcome to Masty o Rasty "The Drunken Truth" p...</td><td>142</td><td><a href="https://open.spotify.com/show/35RtCrgybsUG3dos">https://open.spotify.com/show/35RtCrgybsUG3dos</a>...</td></tr><tr><th>3</th><td> </td><td>Rokh Podcast</td><td>         </td><td>22</td><td><a href="https://open.spotify.com/show/0hDXe6EN56UZsPBr">https://open.spotify.com/show/0hDXe6EN56UZsPBr</a>...</td></tr><tr><th>4</th><td>Radio Deev /   </td><td>RadioDeev</td><td>        ...</td><td>32</td><td><a href="https://open.spotify.com/show/1KZXcjPkHVeaziGY">https://open.spotify.com/show/1KZXcjPkHVeaziGY</a>...</td></tr><tr><th>5</th><td>   - KetabCast</td><td>  - KetabCast</td><td>  - Ketab Cast      ...</td><td>534</td><td><a href="https://open.spotify.com/show/1eoeo5t8CfjoucLo">https://open.spotify.com/show/1eoeo5t8CfjoucLo</a>...</td></tr><tr><th>6</th><td> </td><td>arameshpodcast.com</td><td>        ...</td><td>29</td><td><a href="https://open.spotify.com/show/7cr35mqkp0UbVPCn">https://open.spotify.com/show/7cr35mqkp0UbVPCn</a>...</td></tr><tr><th>7</th><td>BPLUS     </td><td>Ali Bandari</td><td>   </td><td>63</td><td><a href="https://open.spotify.com/show/5HqY5kOdUaGvmQsW">https://open.spotify.com/show/5HqY5kOdUaGvmQsW</a>...</td></tr><tr><th>8</th><td>Digesttt/  </td><td> </td><td>       ...</td><td>41</td><td><a href="https://open.spotify.com/show/0APHFnyp6hB6de0s">https://open.spotify.com/show/0APHFnyp6hB6de0s</a>...</td></tr><tr><th>9</th><td>   </td><td>Mahdi Pourbaqi</td><td>         ...</td><td>79</td><td><a href="https://open.spotify.com/show/5oJxOMLttosMDiPu">https://open.spotify.com/show/5oJxOMLttosMDiPu</a>...</td></tr><tr><th>10</th><td>   </td><td>Ardeshir Tayebi</td><td>        </td><td>50</td><td><a href="https://open.spotify.com/show/7bCiopnv1MlEl2UE">https://open.spotify.com/show/7bCiopnv1MlEl2UE</a>...</td></tr><tr><th>11</th><td>meditation podcast |  </td><td>meditation podcast</td><td>       ...</td><td>58</td><td><a href="https://open.spotify.com/show/2KqzlJNguHFSjx73">https://open.spotify.com/show/2KqzlJNguHFSjx73</a>...</td></tr><tr><th>12</th><td>On podcast  </td><td>mersen</td><td>         ...</td><td>20</td><td><a href="https://open.spotify.com/show/18dEbxRMhmCOSBLB">https://open.spotify.com/show/18dEbxRMhmCOSBLB</a>...</td></tr><tr><th>13</th><td>owrsi |  </td><td>Saman Karampour</td><td>    </td><td>11</td><td><a href="https://open.spotify.com/show/5G5S9nV9WaMVbsJh">https://open.spotify.com/show/5G5S9nV9WaMVbsJh</a>...</td></tr><tr><th>14</th><td>   / Navcast/  ...</td><td>Roshan Abady</td><td>         ...</td><td>74</td><td><a href="https://open.spotify.com/show/2r3S4hgcs0ksj5V9">https://open.spotify.com/show/2r3S4hgcs0ksj5V9</a>...</td></tr><tr><th>15</th><td> </td><td>Mehdi Abbasi</td><td>   </td><td>34</td><td><a href="https://open.spotify.com/show/3a6TjdLquDQvSXtV">https://open.spotify.com/show/3a6TjdLquDQvSXtV</a>...</td></tr><tr><th>16</th><td>Moniaz Podcast |   </td><td>moniaz</td><td>         ...</td><td>47</td><td><a href="https://open.spotify.com/show/2pCF0JIZOagq3OJS">https://open.spotify.com/show/2pCF0JIZOagq3OJS</a>...</td></tr><tr><th>17</th><td>R.O.Tik |  </td><td>R.O.</td><td>   !   ...</td><td>7</td><td><a href="https://open.spotify.com/show/7gFfKTEEE0ILmTLP">https://open.spotify.com/show/7gFfKTEEE0ILmTLP</a>...</td></tr><tr><th>18</th><td>Saate Sefr |    </td><td>Amin Matin |  </td><td>       | * : ...</td><td>39</td><td><a href="https://open.spotify.com/show/6ZbOqYS6h8Q4e2h2">https://open.spotify.com/show/6ZbOqYS6h8Q4e2h2</a>...</td></tr><tr><th>19</th><td>Ravi |   </td><td>arash kaviani</td><td>         ...</td><td>23</td><td><a href="https://open.spotify.com/show/6YpWbAA0PL9A3jEF">https://open.spotify.com/show/6YpWbAA0PL9A3jEF</a>...</td></tr><tr><th>20</th><td>     |  </td><td>Jadi</td><td>    </td><td>20</td><td><a href="https://open.spotify.com/show/2la9aW3sYTNjxVaa">https://open.spotify.com/show/2la9aW3sYTNjxVaa</a>...</td></tr></tbody></table></div>
<h2 id="shows-containing-word-">Shows containing word ''</h2>
<pre><code class="language-python">slice = df[df['Description'].str.contains('')]
slice
</code></pre>
<div><table border="1" class="dataframe"><thead><tr><th></th><th>Name</th><th>Publisher</th><th>Description</th><th>Total Episodes</th><th>URL</th></tr></thead><tbody><tr><th>6</th><td> </td><td>arameshpodcast.com</td><td>        ...</td><td>29</td><td><a href="https://open.spotify.com/show/7cr35mqkp0UbVPCn">https://open.spotify.com/show/7cr35mqkp0UbVPCn</a>...</td></tr><tr><th>11</th><td>meditation podcast |  </td><td>meditation podcast</td><td>       ...</td><td>58</td><td><a href="https://open.spotify.com/show/2KqzlJNguHFSjx73">https://open.spotify.com/show/2KqzlJNguHFSjx73</a>...</td></tr><tr><th>38</th><td>    | Dharma Meditation...</td><td>Ali Delshad</td><td>         ...</td><td>100</td><td><a href="https://open.spotify.com/show/0VcAShPf0PhYeE1w">https://open.spotify.com/show/0VcAShPf0PhYeE1w</a>...</td></tr><tr><th>108</th><td>    | Dharma Meditation...</td><td>Ali Delshad</td><td>          ...</td><td>14</td><td><a href="https://open.spotify.com/show/5mLXV3uB5GX6ZHct">https://open.spotify.com/show/5mLXV3uB5GX6ZHct</a>...</td></tr><tr><th>165</th><td>      | D...</td><td>Ali Delshad</td><td>        ...</td><td>6</td><td><a href="https://open.spotify.com/show/0aslEbqmSj69VDRE">https://open.spotify.com/show/0aslEbqmSj69VDRE</a>...</td></tr><tr><th>192</th><td> </td><td>  </td><td>       ...</td><td>7</td><td><a href="https://open.spotify.com/show/6ZKqF9FLQyo4ZurD">https://open.spotify.com/show/6ZKqF9FLQyo4ZurD</a>...</td></tr><tr><th>339</th><td>      | Dharma...</td><td>Ali Delshad</td><td>       ...</td><td>19</td><td><a href="https://open.spotify.com/show/2ptVB562q1m0hBo4">https://open.spotify.com/show/2ptVB562q1m0hBo4</a>...</td></tr><tr><th>783</th><td>YogaLifeRomane     </td><td>Romaneh Khalili Pour</td><td>           ...</td><td>14</td><td><a href="https://open.spotify.com/show/0fu00jdnh2Cn3pS8">https://open.spotify.com/show/0fu00jdnh2Cn3pS8</a>...</td></tr><tr><th>870</th><td>Asoothe | </td><td>Araz Pourvatan</td><td>Life is an experiment. Here the experience is ...</td><td>3</td><td><a href="https://open.spotify.com/show/2HGdWKcdo6Sj32wn">https://open.spotify.com/show/2HGdWKcdo6Sj32wn</a>...</td></tr><tr><th>947</th><td>RAZPAD</td><td> </td><td>.         ...</td><td>9</td><td><a href="https://open.spotify.com/show/6HiN7mGLCz7D1A8c">https://open.spotify.com/show/6HiN7mGLCz7D1A8c</a>...</td></tr></tbody></table></div>
<h2 id="statistics">Statistics</h2>
<pre><code class="language-python">df.describe()
</code></pre>
<div><table border="1" class="dataframe"><thead><tr><th></th><th>Total Episodes</th></tr></thead><tbody><tr><th>count</th><td>1000.000000</td></tr><tr><th>mean</th><td>27.484000</td></tr><tr><th>std</th><td>75.644338</td></tr><tr><th>min</th><td>0.000000</td></tr><tr><th>25%</th><td>4.000000</td></tr><tr><th>50%</th><td>10.000000</td></tr><tr><th>75%</th><td>25.000000</td></tr><tr><th>max</th><td>1283.000000</td></tr></tbody></table></div>
<pre><code class="language-python">df['Total Episodes'].value_counts()
</code></pre>
<p>1      105
2       69
4       57
3       55
5       52
...
114      1
115      1
116      1
136      1
0        1
Name: Total Episodes, Length: 126, dtype: int64</p>
<h2 id="with-more-than-400-episodes">With more than 400 Episodes</h2>
<pre><code class="language-python">df[df['Total Episodes'] &gt; 400]
</code></pre>
<div><table border="1" class="dataframe"><thead><tr><th></th><th>Name</th><th>Publisher</th><th>Description</th><th>Total Episodes</th><th>URL</th></tr></thead><tbody><tr><th>5</th><td>   - KetabCast</td><td>  - KetabCast</td><td>  - Ketab Cast      ...</td><td>534</td><td><a href="https://open.spotify.com/show/1eoeo5t8CfjoucLo">https://open.spotify.com/show/1eoeo5t8CfjoucLo</a>...</td></tr><tr><th>54</th><td>Radio Padio |   </td><td>RadioPadio |   </td><td>  !    ...</td><td>529</td><td><a href="https://open.spotify.com/show/47ITpNV6wVBmF0Hw">https://open.spotify.com/show/47ITpNV6wVBmF0Hw</a>...</td></tr><tr><th>469</th><td> </td><td>  </td><td>       ...</td><td>833</td><td><a href="https://open.spotify.com/show/7FiQjYyDfZtZHFbU">https://open.spotify.com/show/7FiQjYyDfZtZHFbU</a>...</td></tr><tr><th>493</th><td>   </td><td>Plato</td><td>      1  . ...</td><td>1073</td><td><a href="https://open.spotify.com/show/5r5dc2JA7Y56ZYFd">https://open.spotify.com/show/5r5dc2JA7Y56ZYFd</a>...</td></tr><tr><th>550</th><td>  | mihantv</td><td>mihantv1</td><td>            ...</td><td>1283</td><td><a href="https://open.spotify.com/show/7cPVgVgTb2PAnysU">https://open.spotify.com/show/7cPVgVgTb2PAnysU</a>...</td></tr><tr><th>939</th><td></td><td> </td><td>        ...</td><td>430</td><td><a href="https://open.spotify.com/show/3BjE3M4G87Yri4Ot">https://open.spotify.com/show/3BjE3M4G87Yri4Ot</a>...</td></tr></tbody></table></div>
<pre><code class="language-python">df[df['Total Episodes'] &lt; 10]
</code></pre>
<div><table border="1" class="dataframe"><thead><tr><th></th><th>Name</th><th>Publisher</th><th>Description</th><th>Total Episodes</th><th>URL</th></tr></thead><tbody><tr><th>17</th><td>R.O.Tik |  </td><td>R.O.</td><td>   !   <em></em><strong></strong>...</td><td>7</td><td><a href="https://open.spotify.com/show/7gFfKTEEE0ILmTLP">https://open.spotify.com/show/7gFfKTEEE0ILmTLP</a>...</td></tr><tr><th>42</th><td>  |     </td><td>  </td><td>         ...</td><td>5</td><td><a href="https://open.spotify.com/show/5c27xgMZdXaSzAxb">https://open.spotify.com/show/5c27xgMZdXaSzAxb</a>...</td></tr><tr><th>59</th><td>  | Hoshtak</td><td>MohammadSalar</td><td>         ...</td><td>8</td><td><a href="https://open.spotify.com/show/4Fhy3dfYAlkB5Nth">https://open.spotify.com/show/4Fhy3dfYAlkB5Nth</a>...</td></tr><tr><th>61</th><td>Mind Master </td><td>Sara</td><td>        ...</td><td>8</td><td><a href="https://open.spotify.com/show/5G2T1M7mSbcvY3d1">https://open.spotify.com/show/5G2T1M7mSbcvY3d1</a>...</td></tr><tr><th>68</th><td>salam podcast |   </td><td>basalam</td><td>      </td><td>8</td><td><a href="https://open.spotify.com/show/7c7QY4VEU3MjDT9Q">https://open.spotify.com/show/7c7QY4VEU3MjDT9Q</a>...</td></tr><tr><th>...</th><td>...</td><td>...</td><td>...</td><td>...</td><td>...</td></tr><tr><th>995</th><td> | Sabketo ()</td><td>Sabketo</td><td>     ...</td><td>0</td><td><a href="https://open.spotify.com/show/7wMHG6jwpMg4OWxF">https://open.spotify.com/show/7wMHG6jwpMg4OWxF</a>...</td></tr><tr><th>997</th><td>  </td><td>Farzad Bayan</td><td>  </td><td>1</td><td><a href="https://open.spotify.com/show/5WRbEroh1Wzb5bvW">https://open.spotify.com/show/5WRbEroh1Wzb5bvW</a>...</td></tr><tr><th>998</th><td>NightPods -  </td><td>NoN Residential</td><td>         ...</td><td>5</td><td><a href="https://open.spotify.com/show/1o2Kvl0Q0rrOCVEZ">https://open.spotify.com/show/1o2Kvl0Q0rrOCVEZ</a>...</td></tr><tr><th>999</th><td>:     </td><td>1343</td><td>        ...</td><td>2</td><td><a href="https://open.spotify.com/show/5ZAb7t3FF1A7eKrD">https://open.spotify.com/show/5ZAb7t3FF1A7eKrD</a>...</td></tr><tr><th>1000</th><td> </td><td>Hesam Bokazadeh</td><td>         ...</td><td>9</td><td><a href="https://open.spotify.com/show/0IOjqr6qtaaFw9B4">https://open.spotify.com/show/0IOjqr6qtaaFw9B4</a>...</td></tr></tbody></table><p>486 rows  5 columns</p></div>]]></content>
        <author>
            <name>Amir Afshari</name>
            <uri>https://github.com/amirafshari</uri>
        </author>
        <category label="Data Analysis" term="Data Analysis"/>
        <category label="API" term="API"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Housing Data Analysis]]></title>
        <id>https://amirafshari.github.io/blog/housing-data-analysis</id>
        <link href="https://amirafshari.github.io/blog/housing-data-analysis"/>
        <updated>2020-08-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[density2]]></summary>
        <content type="html"><![CDATA[<p><img src="https://user-images.githubusercontent.com/17769927/138718424-6f6cd0c6-c5a6-4442-a058-80968fc29035.png" alt="density2"></p>
<p>This dataset is based on data from the 1990 California census. Our dataset consist of 26k samples with 10 features.</p>
<p>features = [longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, median_house_value, ocean_proximity]</p>
<h2 id="ocean-proximity-count">Ocean Proximity Count</h2>
<p>Ocean Proximity is our only categorical data in this dataset.</p>
<p><img src="https://user-images.githubusercontent.com/17769927/138718362-f419fed8-b34c-41cb-9cb2-09f3ce13e5e5.png" alt="ocean_proximity">
<img src="https://user-images.githubusercontent.com/17769927/138718371-c17dede1-247e-4090-b54f-6d650121d9cb.png" alt="ocean_proximity-pie"></p>
<h2 id="where-are-the-most-populated-areas">Where are the most populated areas?</h2>
<h3 id="population-density-recongnition">Population Density Recongnition</h3>
<p><img src="https://user-images.githubusercontent.com/17769927/138718446-ca1aab50-45b3-4094-86ef-bcb9a4fb07fc.png" alt="density">
<img src="https://user-images.githubusercontent.com/17769927/138718413-3e29fa00-2f4f-4967-a752-39ce56fb455b.png" alt="density1">
<img src="https://user-images.githubusercontent.com/17769927/138718424-6f6cd0c6-c5a6-4442-a058-80968fc29035.png" alt="density2"></p>
<h2 id="correlations">Correlations</h2>
<h3 id="median-income-vs-median-house-value-strongest-correlation">Median Income vs Median House Value (Strongest Correlation)</h3>
<p><img src="https://user-images.githubusercontent.com/17769927/138719144-34f2ed03-e332-4646-811b-3569a8d5f7ae.png" alt="correlations"></p>
<h2 id="distribution-of-features">Distribution of Features</h2>
<p><img src="https://user-images.githubusercontent.com/17769927/138719163-09cf03d6-64e9-409e-b647-66529e2abb73.png" alt="feature-distribution"></p>
<h2 id="outliers">Outliers</h2>
<p><img src="https://user-images.githubusercontent.com/17769927/138719182-5addddd8-398f-4cb5-9bd7-b293820d9ff2.png" alt="outliers"></p>]]></content>
        <author>
            <name>Amir Afshari</name>
            <uri>https://github.com/amirafshari</uri>
        </author>
        <category label="Data Analysis" term="Data Analysis"/>
        <category label="Data Visualization" term="Data Visualization"/>
    </entry>
</feed>