<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.1">
<title data-rh="true">Satellite Data Vector Database | Amir Afshari</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://amirafshari.github.io/blog/remote-sensing-image-search-cbir-becnchmark"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Satellite Data Vector Database | Amir Afshari"><meta data-rh="true" name="description" content="png"><meta data-rh="true" property="og:description" content="png"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2024-05-18T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/amirafshari"><meta data-rh="true" property="article:tag" content="Deep Learning,Machine Learning,Computer Vision,Remote Sensing"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://amirafshari.github.io/blog/remote-sensing-image-search-cbir-becnchmark"><link data-rh="true" rel="alternate" href="https://amirafshari.github.io/blog/remote-sensing-image-search-cbir-becnchmark" hreflang="en"><link data-rh="true" rel="alternate" href="https://amirafshari.github.io/blog/remote-sensing-image-search-cbir-becnchmark" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Amir Afshari RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Amir Afshari Atom Feed"><link rel="stylesheet" href="/assets/css/styles.8f1f9da0.css">
<script src="/assets/js/runtime~main.3dfe0e8b.js" defer="defer"></script>
<script src="/assets/js/main.dbc06eb6.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"dark")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_gu5v" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><b class="navbar__title text--truncate">Home</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/feed">Brain Feed</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/amirafshari" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Github</a><a href="https://www.linkedin.com/in/amirafshari/" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Linkedin</a><div class="toggle_kWbt colorModeToggle_GwZs"><button class="clean-btn toggleButton_fOL9 toggleButtonDisabled_STpu" type="button" disabled="" title="Switch between dark and light mode (currently dark mode)" aria-label="Switch between dark and light mode (currently dark mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_DCeJ"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_DFgp"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_IP3a"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_IbdI"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_GnOS thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_aARK margin-bottom--md">Recent posts</div><ul class="sidebarItemList_a8Ne clean-list"><li class="sidebarItem_Otbb"><a aria-current="page" class="sidebarItemLink_OBo2 sidebarItemLinkActive_guiV" href="/blog/remote-sensing-image-search-cbir-becnchmark">Satellite Data Vector Database</a></li><li class="sidebarItem_Otbb"><a class="sidebarItemLink_OBo2" href="/blog/remote-sensing-image-search-cbir">Multimodal Vision-Language Search on Satellite Images</a></li><li class="sidebarItem_Otbb"><a class="sidebarItemLink_OBo2" href="/blog/license-plate-detector">Automatic License Plate Detector</a></li><li class="sidebarItem_Otbb"><a class="sidebarItemLink_OBo2" href="/blog/train-custom-object-detector">Train a Custom Object Detector</a></li><li class="sidebarItem_Otbb"><a class="sidebarItemLink_OBo2" href="/blog/spotify-persian-podcasts">Persian Podcasts on Spotify</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="https://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="https://schema.org/BlogPosting"><meta itemprop="description" content="png"><header><h1 class="title_xzwX" itemprop="headline">Satellite Data Vector Database</h1><div class="container_HY9_ margin-vert--md"><time datetime="2024-05-18T00:00:00.000Z" itemprop="datePublished">May 18, 2024</time> Â· <!-- -->14 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_uEq3"><div class="avatar margin-bottom--sm"><a href="https://github.com/amirafshari" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://avatars.githubusercontent.com/u/17769927?s=400&amp;u=d630f608970a53d00295f2e87e88526b41b7d0b1&amp;v=4" alt="Amir Afshari" itemprop="image"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/amirafshari" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Amir Afshari</span></a></div><small class="avatar__subtitle" itemprop="description">Machine Learning Engineer</small></div></div></div></div></header><div id="__blog-post-container" class="markdown" itemprop="articleBody"><p><img alt="png" src="/assets/images/image-search_60_1-640dab6c0bee7b9b33e6a35412c6dd34.png" width="800" height="500"></p>
<p>With the rapid advancement of remote sensing technologies, the availability of large-scale satellite image datasets has grown exponentially. These datasets contain invaluable information for various applications, including environmental monitoring, urban planning, and disaster management. However, extracting specific categories of objects, such as identifying all images which are similar to a sepcific query image
within a dataset of thousands or millions or even billions of samples, presents a significant challenge for human analyst.
ships within a dataset of one million samples, presents a significant challenge due to the sheer volume of data and the complexity of manual analysis.</p>
<h3 id="solution">Solution</h3>
<p>This task, which is overwhelming for human analysts, can be efficiently addressed using vector search techniques. By leveraging deep learning models to transform images into high-dimensional vectors and utilizing various models such as classification, segmentation, etc, we can use their last layer features and employ nearest neighbor search algorithms to quickly and accurately retrieve relevant images based on their content or semantic meaning.</p>
<p>For instance, you find and interesting shape in your dataset and you want to figure out if there is any similar image in your dataset or not? To do so, you can use that image as a search query to find the similar images.</p>
<h3 id="good-to-know">Good to know</h3>
<ul>
<li>I just fine-tuned ResNet50 on AID dataset, we can train more architectures on different datasets and benchmark them.</li>
<li>By fine-tuning on a large-scale dataset, the accuracy significantly improves.</li>
<li>It&#x27;s not a production-level (or even near-production-level) solution, so there is plenty of room for improvement in both speed and accuracy.</li>
<li>We can use vector databases such as Weaviate or Redis instead of a simple Python list.</li>
<li>It&#x27;s good to dig into the datasets first and then judge the performance of the model.</li>
<li>You can find the <a href="https://github.com/amirafshari/rs-cbir">code on Github</a></li>
</ul>
<pre><code class="language-python">import matplotlib.pyplot as plt
import seaborn as sns

import pandas as pd
import numpy as np 
import torch
import torch.nn as nn
from torchvision import models, transforms, utils
from torch.utils.data import Dataset, DataLoader, random_split
from skimage import io, transform
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import confusion_matrix, accuracy_score

import os
import glob
import random

from PIL import Image
</code></pre>
<pre><code class="language-python">device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
device
</code></pre>
<p>device(type=&#x27;cuda&#x27;)</p>
<h2 id="datasets">Datasets</h2>
<h3 id="aid">AID</h3>
<p>To Do</p>
<ul>
<li>Dataset Details</li>
</ul>
<pre><code class="language-python">class AID(Dataset):

    def __init__(self, root_dir):

        self.label_map = {}
        self.info = []
        i = 0
        for root, dirs, files in os.walk(root_dir):
                
            for file in files:
                
                if file.endswith((&quot;.jpg&quot;, &quot;.tif&quot;, &quot;.png&quot;, &quot;jpeg&quot;)):
                    file_path = os.path.join(root, file)
                    label = root.split(&#x27;/&#x27;)[-1]
                    self.info.append((file_path, label.lower()))


                if label.lower() not in self.label_map:
                    self.label_map[label.lower()] = i
                    i += 1

        random.shuffle(self.info)
        self.transform = transform


    def map_labels(self, label):
        return self.label_map[label]





    def __len__(self):
        return len(self.info)


    def __getitem__(self, idx):
        img_path, label = self.info[idx]

        label = self.map_labels(label.lower())

        img = io.imread(img_path)[:,:,:3]
        img = img / 255.
        img = torch.tensor(img, dtype=torch.float32)
        
        return img, label, img_path
</code></pre>
<pre><code class="language-python">dataset = AID(&#x27;datasets/AID&#x27;)

label_map = dataset.label_map
class_map = {v: k for k, v in label_map.items()}
classes = [v for k,v in class_map.items()]
</code></pre>
<pre><code class="language-python">rows, columns = 2, 5
fig, axs = plt.subplots(rows, columns, figsize=(21, 8), squeeze=True)


n = 0
for i in range(rows):
    for j in range(columns):
        img, label, _ = dataset[n]
        clss = class_map[label]

        axs[i][j].imshow(img)
        axs[i][j].set_title(clss)
        n += 1
plt.show()
</code></pre>
<p><img alt="png" src="/assets/images/image-search_8_0-25c11ee343a89a4df15c21a610eb0aff.png" width="800" height="300"></p>
<h3 id="fair1m">FAIR1M</h3>
<pre><code class="language-python">class FAIR1M(Dataset):

    def __init__(self, root_dir, transform=None):

        self.label_map = {
            &#x27;ship&#x27;: 0,
            &#x27;airplane&#x27;: 1,
            &#x27;neighborhood&#x27;: 2,
        }

        self.info = []
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                if file.endswith((&quot;.jpg&quot;, &quot;.tif&quot;, &quot;.png&quot;, &quot;jpeg&quot;)):
                    file_path = os.path.join(root, file)
                    label = root.split(&#x27;/&#x27;)[-1]
                    self.info.append((file_path, label.lower()))

        random.shuffle(self.info)
        self.transform = transform

    def map_labels(self, label):
        return self.label_map[label]





    def __len__(self):
        return len(self.info)


    def __getitem__(self, idx):
        img_path, label = self.info[idx]

        label = self.map_labels(label.lower())

        img = io.imread(img_path)[:,:,:3]
        img = img / 255.
        img = torch.tensor(img, dtype=torch.float32)
        
        return img, label, img_path
</code></pre>
<pre><code class="language-python">dataset = FAIR1M(&#x27;datasets/FAIR1M_partial&#x27;)

class_map = {
    0: &#x27;ship&#x27;,
    1: &#x27;airplane&#x27;,
    2: &#x27;neighborhood&#x27;,
}
classes = [v for k,v in class_map.items()]
classes
</code></pre>
<p>[&#x27;ship&#x27;, &#x27;airplane&#x27;, &#x27;neighborhood&#x27;]</p>
<pre><code class="language-python">rows, columns = 2, 5
fig, axs = plt.subplots(rows, columns, figsize=(21, 8), squeeze=True)


n = 0
for i in range(rows):
    for j in range(columns):
        img, label, _ = dataset[n]
        clss = class_map[label]

        axs[i][j].imshow(img)
        axs[i][j].set_title(clss)
        n += 1
plt.show()
</code></pre>
<p><img alt="png" src="/assets/images/image-search_13_0-13d2af7f5f68dc6d50f351728fd8ddf6.png" width="800" height="300"></p>
<h3 id="resisc45">RESISC45</h3>
<pre><code class="language-python">class RESISC45(Dataset):

    def __init__(self, root_dir, transform=None):

        self.label_map = {
            &#x27;ship&#x27;: 0,
            &#x27;airplane&#x27;: 1,
            &#x27;bridge&#x27;: 2,
        }

        self.info = []
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                if file.endswith((&quot;.jpg&quot;, &quot;.tif&quot;, &quot;.png&quot;, &quot;jpeg&quot;)):
                    file_path = os.path.join(root, file)
                    label = root.split(&#x27;/&#x27;)[-1]
                    self.info.append((file_path, label))

        random.shuffle(self.info)
        self.transform = transform

    def map_labels(self, label):
        return self.label_map[label]





    def __len__(self):
        return len(self.info)


    def __getitem__(self, idx):
        img_path, label = self.info[idx]

        label = self.map_labels(label.lower())

        img = io.imread(img_path)[:,:,:3]
        img = img / 255.
        img = torch.tensor(img, dtype=torch.float32)
        
        return img, label, img_path
</code></pre>
<pre><code class="language-python">dataset = RESISC45(&#x27;datasets/RESISC45_partial&#x27;)
</code></pre>
<pre><code class="language-python">class_map = {
    0: &#x27;ship&#x27;,
    1: &#x27;airplane&#x27;,
    2: &#x27;bridge&#x27;,
}
classes = [v for k,v in class_map.items()]
classes
</code></pre>
<p>[&#x27;ship&#x27;, &#x27;airplane&#x27;, &#x27;bridge&#x27;]</p>
<pre><code class="language-python">rows, columns = 2, 5
fig, axs = plt.subplots(rows, columns, figsize=(21, 8), squeeze=True)


n = 0
for i in range(rows):
    for j in range(columns):
        img, label, _ = dataset[n]
        clss = class_map[label]

        axs[i][j].imshow(img)
        axs[i][j].set_title(clss)
        n += 1
plt.show()
</code></pre>
<p><img alt="png" src="/assets/images/image-search_18_0-12cbcd9abf2444e3218ae01195aabf52.png" width="800" height="300"></p>
<h3 id="sentinel-2-ship">Sentinel-2 ship</h3>
<pre><code class="language-python">class SS2(Dataset):

    def __init__(self, root_dir, transform=None):

        self.label_map = {
            &#x27;ship&#x27;: 0,
            &#x27;noship&#x27;: 1,
        }

        self.info = []
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                if file.endswith((&quot;.jpg&quot;, &quot;.tif&quot;, &quot;.png&quot;, &quot;jpeg&quot;)):
                    file_path = os.path.join(root, file)
                    label = root.split(&#x27;/&#x27;)[-1]
                    self.info.append((file_path, label))

        random.shuffle(self.info)
        self.transform = transform

    def map_labels(self, label):
        return self.label_map[label]





    def __len__(self):
        return len(self.info)


    def __getitem__(self, idx):
        img_path, label = self.info[idx]

        label = self.map_labels(label.lower())

        img = io.imread(img_path)[:,:,:3]
        img = img / 255.
        img = torch.tensor(img, dtype=torch.float32)
        
        return img, label, img_path
</code></pre>
<pre><code class="language-python">dataset = SS2(&#x27;datasets/Sentinel2_partial&#x27;)
</code></pre>
<pre><code class="language-python">class_map = {
    0: &#x27;ship&#x27;,
    1: &#x27;noship&#x27;,
}
classes = [v for k,v in class_map.items()]
classes
</code></pre>
<p>[&#x27;ship&#x27;, &#x27;noship&#x27;]</p>
<pre><code class="language-python">rows, columns = 2, 5
fig, axs = plt.subplots(rows, columns, figsize=(21, 8), squeeze=True)


n = 0
for i in range(rows):
    for j in range(columns):
        img, label, _ = dataset[n]
        clss = class_map[label]

        axs[i][j].imshow(img)
        axs[i][j].set_title(clss)
        n += 1
plt.show()
</code></pre>
<p><img alt="png" src="/assets/images/image-search_23_0-09e7da946801ed88c33a4bfde78c9efc.png" width="800" height="300"></p>
<h2 id="models">Models</h2>
<ul>
<li><a href="https://pytorch.org/vision/stable/models.html">https://pytorch.org/vision/stable/models.html</a></li>
<li><a href="https://pytorch.org/vision/main/models">https://pytorch.org/vision/main/models</a></li>
<li>load your neural netwrok for feature extraction</li>
</ul>
<p>To Do</p>
<ul>
<li>Use fine-tuned weights</li>
<li>Use different architectures for this task and compare their infernce speed and accuracy.</li>
</ul>
<pre><code class="language-python">weights = [&#x27;IMAGENET1K_V1&#x27;, &#x27;IMAGENET1K_V2&#x27;, &#x27;IMAGENET1K_SWAG_E2E_V1&#x27;, &#x27;IMAGENET1K_SWAG_E2E_V1&#x27;, &#x27;IMAGENET1K_SWAG_LINEAR_V1&#x27;]
</code></pre>
<p>This is a simple way to load a pre-trained model using PyTorch</p>
<pre><code class="language-python">model = torch.hub.load(&quot;pytorch/vision&quot;, &#x27;resnet50&#x27;, weights=&quot;IMAGENET1K_V2&quot;)
</code></pre>
<h3 id="swin">Swin</h3>
<p>Pre-trained on ImageNet</p>
<pre><code class="language-python"># Swin
from torchvision.models import swin_v2_b, Swin_B_Weights

model = models.swin_v2_b(weights=Swin_B_Weights).to(device).eval()

total_params = &#x27;{:,}&#x27;.format(sum(p.numel() for p in model.parameters()))
total_params
</code></pre>
<pre><code class="language-python">model
</code></pre>
<h3 id="resnet">ResNet</h3>
<p>Pre-trained on ImageNet</p>
<pre><code class="language-python"># model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2).to(device).eval()
# model = models.resnet50(pretrained=True)
</code></pre>
<p>Fine-tuned on AID</p>
<pre><code class="language-python">model = models.resnet50(pretrained=False)
</code></pre>
<p>/home/amir/miniconda3/envs/faiss/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter &#x27;pretrained&#x27; is deprecated since 0.13 and may be removed in the future, please use &#x27;weights&#x27; instead.
warnings.warn(
/home/amir/miniconda3/envs/faiss/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or <code>None</code> for &#x27;weights&#x27; are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing <code>weights=None</code>.
warnings.warn(msg)</p>
<pre><code class="language-python">model
</code></pre>
<p>Modify the last layer based on your fine-tuned weights</p>
<pre><code class="language-python">num_classes = 17 # AID
# num_classes = 43 # BigEarthNet

model.fc = torch.nn.Linear(model.fc.in_features, num_classes)
</code></pre>
<pre><code class="language-python">checkpoint = torch.load(&#x27;weights/aid_multilabel_scratch_resnet50.pth&#x27;, map_location=torch.device(&#x27;cpu&#x27;))
state_dict = checkpoint[&#x27;state_dict&#x27;]
model.load_state_dict(state_dict)
model.to(device).eval()
</code></pre>
<pre><code>    ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Linear(in_features=2048, out_features=17, bias=True)
    )
</code></pre>
<p>Using this module, we can extract features from the last layer of our network, to use it as our feature extractor for our vector database</p>
<pre><code class="language-python">class ResNetFeatures(torch.nn.Module):
    def __init__(self, original_model):
        super(ResNetFeatures, self).__init__()
        self.features = torch.nn.Sequential(*list(original_model.children())[:-1])  # all layers except the final FC layer
    
    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, 1)
        return x

# Create an instance of the new model
model = ResNetFeatures(model).to(device).eval()
</code></pre>
<pre><code class="language-python">for key in state_dict.keys():
    print(key)
</code></pre>
<p>Number of model parameters</p>
<pre><code class="language-python">total_params = &#x27;{:,}&#x27;.format(sum(p.numel() for p in model.parameters()))
total_params
</code></pre>
<p>&#x27;23,508,032&#x27;</p>
<h3 id="regnet">RegNet</h3>
<pre><code class="language-python"># RegNet

model = models.regnet_y_400mf(pretrained=True).to(device).eval()

total_params = &#x27;{:,}&#x27;.format(sum(p.numel() for p in model.parameters()))
total_params
</code></pre>
<pre><code class="language-python">model
</code></pre>
<h2 id="metrics">Metrics</h2>
<p>To Do</p>
<ul>
<li>What metrics for CBIR?</li>
</ul>
<pre><code class="language-python">def accuracy(query_label, neighbors, labels):
    &#x27;&#x27;&#x27; A simple function to calculate the accuracy for 1 sample @ K&#x27;&#x27;&#x27;
    t, f = 0, 0
    for i in neighbors:
        if query_label == labels[i]:
            t += 1
        else:
            f += 1
    return &#x27;{:.1%}&#x27;.format(t / (t + f))
</code></pre>
<h2 id="vector-database">Vector Database</h2>
<p>Here we use our feature extractor to convert all of our images to a (1, 2048) vector and store them in our vector database (a python list in here) and perform our NN search.</p>
<p>Of course it&#x27;s not implemented for production level use, but we can scale it using Vector Database such as Weaviate or Pinecone, etc. We can also use simpler implementation with FAISS library for our vector database and NN search.</p>
<pre><code class="language-python">features = []
image_paths = []
labels = []


# Change the first line according to the dataset you want to use
for x, y, img_path in dataset:
    img = x.permute(2, 0, 1).unsqueeze(0)
    
    with torch.no_grad():
        feature = model(img.to(device)).detach().cpu().numpy()

    features.append(feature)
    image_paths.append(img_path)
    labels.append(y)


features = np.concatenate(features, axis=0)
</code></pre>
<pre><code class="language-python">features.shape
</code></pre>
<p>(10000, 2048)</p>
<p><strong>Swin</strong></p>
<ul>
<li>took</li>
</ul>
<p><strong>ResNet</strong></p>
<ul>
<li>Took 4m 20s for AID on NVIDIA 3060</li>
</ul>
<h2 id="nn-search">NN Search</h2>
<pre><code class="language-python">id = random.randint(0, len(dataset))
img, label, img_path = dataset[id]
img = img.permute(2, 0, 1).unsqueeze(0)

query_image_path = img_path
query_label = label

with torch.no_grad():
    query_feature = model(img.to(device)).detach().cpu().numpy()
</code></pre>
<pre><code class="language-python">k = 20

neigh = NearestNeighbors(n_neighbors=k, algorithm=&#x27;brute&#x27;)
neigh.fit(features)
distances, indices = neigh.kneighbors(query_feature)
</code></pre>
<pre><code class="language-python"># Plot query image
query_image = Image.open(query_image_path)


print(f&#x27;Accuracy for this sample @ k = {k}: {accuracy(query_label, indices[0], labels)}&#x27;)

rows, columns = 4, 5
fig, axs = plt.subplots(rows, columns, figsize=(34, 21))

n = 0
for i in range(rows):
    for j in range(columns):
        if (i == 0) and (j == 0):
            axs[0][0].imshow(query_image)
            axs[0][0].set_title(f&#x27;Query: {class_map[query_label]}&#x27;)
        else:
            axs[i][j].imshow(Image.open(image_paths[indices[0][n+1]]))
            axs[i][j].set_title(f&#x27;NN {n+1}: {class_map[labels[indices[0][n+1]]]}&#x27;)
            n += 1
</code></pre>
<p>Accuracy for this sample @ k = 20: 100.0%</p>
<p><img alt="png" src="/assets/images/image-search_60_1-640dab6c0bee7b9b33e6a35412c6dd34.png" width="800" height="500"></p>
<h2 id="evaluation">Evaluation</h2>
<p>We used the AID dataset for both training and inference</p>
<pre><code class="language-python">def compute_confusion_matrix_and_accuracy(dataset, model, device, features, labels, ks):
    # Initialize a dictionary to hold confusion matrices and accuracies for different k
    results = {k: {&#x27;confusion_matrix&#x27;: None, &#x27;accuracy&#x27;: None, &#x27;per_class_accuracy&#x27;: None, &#x27;confusion_matrix_accuracy&#x27;: None} for k in ks}
    
    # Initialize the NearestNeighbors model
    neigh = NearestNeighbors(n_neighbors=max(ks), algorithm=&#x27;brute&#x27;)
    neigh.fit(features)
    
    # Prepare ground truth and predicted labels for confusion matrix calculation
    all_true_labels = []
    all_pred_labels = {k: [] for k in ks}
    
    for i in range(len(dataset)):
        img, label, _ = dataset[i]
        img = img.permute(2, 0, 1).unsqueeze(0)

        with torch.no_grad():
            query_feature = model(img.to(device)).detach().cpu().numpy()

        distances, indices = neigh.kneighbors(query_feature)
        
        true_label = labels[i]
        all_true_labels.append(true_label)
        
        for k in ks:
            neighbors = indices[0][:k]
            predicted_label = np.bincount([labels[n] for n in neighbors]).argmax()
            all_pred_labels[k].append(predicted_label)
    
    # Calculate confusion matrices and accuracies for each k
    for k in ks:
        cm = confusion_matrix(all_true_labels, all_pred_labels[k])
        acc = accuracy_score(all_true_labels, all_pred_labels[k])

        results[k][&#x27;confusion_matrix&#x27;] = cm
        results[k][&#x27;accuracy&#x27;] = acc
        # results[k][&#x27;per_class_accuracy&#x27;] = cm.diagonal() / cm.sum(axis=0)

        conf_mat_acc = np.zeros_like(cm, dtype=float)
        # results[k][&#x27;confusion_matrix&#x27;].dtype = float
        for i, row in enumerate(cm):
            conf_mat_acc[i] = row / row.sum()
        results[k][&#x27;confusion_matrix_accuracy&#x27;] = conf_mat_acc
    

    
    return results
</code></pre>
<pre><code class="language-python">def visualize_confusion_matrix(cm, class_names, k):
    plt.figure(figsize=(18, 18))  # Increase figure size
    sns.set(font_scale=1.2)  # Increase font scale
    sns.heatmap(cm, annot=True, fmt=&#x27;&#x27;, cmap=&#x27;Blues&#x27;, xticklabels=class_names, yticklabels=class_names, annot_kws={&quot;size&quot;: 12, &quot;ha&quot;: &#x27;center&#x27;, &quot;va&quot;: &#x27;center&#x27;}, linewidths=.5, linecolor=&#x27;black&#x27;)
    plt.xlabel(&#x27;Predicted Label&#x27;)
    plt.ylabel(&#x27;True Label&#x27;)
    plt.title(f&#x27;Predictions for k={k}&#x27;)
    plt.show()
</code></pre>
<pre><code class="language-python">ks = [10, 50, 100]
results = compute_confusion_matrix_and_accuracy(dataset, model, device, features, labels, ks)
</code></pre>
<h3 id="accuracies-confusion-matrix">Accuracies Confusion Matrix</h3>
<pre><code class="language-python">for k in ks:
    visualize_confusion_matrix(np.round(results[k][&#x27;confusion_matrix_accuracy&#x27;], 2), classes, k)
</code></pre>
<p><img alt="png" src="/assets/images/image-search_66_0-63ce4112af9273a7a683d2b1bd74290c.png" width="800" height="800"></p>
<p><img alt="png" src="/assets/images/image-search_66_1-295bc34f26651e527ead3f3151ee24e8.png" width="800" height="800"></p>
<p><img alt="png" src="/assets/images/image-search_66_2-f3005aba9d4fc86cbf7137f7bf44efc3.png" width="800" height="800"></p>
<h3 id="predictions-confusion-matrix">Predictions Confusion Matrix</h3>
<pre><code class="language-python">for k in ks:
    visualize_confusion_matrix(results[k][&#x27;confusion_matrix&#x27;], classes, k)
</code></pre>
<p><img alt="png" src="/assets/images/image-search_68_0-e6821aa1e926207e49eb3f59f0410da2.png" width="800" height="800"></p>
<p><img alt="png" src="/assets/images/image-search_68_1-5c43d7105b41e4da8500c2632af29516.png" width="800" height="800"></p>
<p><img alt="png" src="/assets/images/image-search_68_2-61146bbdcc9dfa033d6d0abb7213cbaf.png" width="800" height="800"></p></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_vccZ"><div class="col"><b>Tags:</b><ul class="tags_aHIs padding--none margin-left--sm"><li class="tag_nwHU"><a class="tag_QDqo tagRegular_RTiO" href="/blog/tags/deep-learning">Deep Learning</a></li><li class="tag_nwHU"><a class="tag_QDqo tagRegular_RTiO" href="/blog/tags/machine-learning">Machine Learning</a></li><li class="tag_nwHU"><a class="tag_QDqo tagRegular_RTiO" href="/blog/tags/computer-vision">Computer Vision</a></li><li class="tag_nwHU"><a class="tag_QDqo tagRegular_RTiO" href="/blog/tags/remote-sensing">Remote Sensing</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.com/amirafshari/amirafshari.github.io/tree/main/blog/2024-05-18-remote-sensing-image-search-benchmark.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_NulP" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/blog/remote-sensing-image-search-cbir"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Multimodal Vision-Language Search on Satellite Images</div></a></nav></main><div class="col col--2"><div class="tableOfContents_IS5x thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#solution" class="table-of-contents__link toc-highlight">Solution</a></li><li><a href="#good-to-know" class="table-of-contents__link toc-highlight">Good to know</a></li><li><a href="#datasets" class="table-of-contents__link toc-highlight">Datasets</a><ul><li><a href="#aid" class="table-of-contents__link toc-highlight">AID</a></li><li><a href="#fair1m" class="table-of-contents__link toc-highlight">FAIR1M</a></li><li><a href="#resisc45" class="table-of-contents__link toc-highlight">RESISC45</a></li><li><a href="#sentinel-2-ship" class="table-of-contents__link toc-highlight">Sentinel-2 ship</a></li></ul></li><li><a href="#models" class="table-of-contents__link toc-highlight">Models</a><ul><li><a href="#swin" class="table-of-contents__link toc-highlight">Swin</a></li><li><a href="#resnet" class="table-of-contents__link toc-highlight">ResNet</a></li><li><a href="#regnet" class="table-of-contents__link toc-highlight">RegNet</a></li></ul></li><li><a href="#metrics" class="table-of-contents__link toc-highlight">Metrics</a></li><li><a href="#vector-database" class="table-of-contents__link toc-highlight">Vector Database</a></li><li><a href="#nn-search" class="table-of-contents__link toc-highlight">NN Search</a></li><li><a href="#evaluation" class="table-of-contents__link toc-highlight">Evaluation</a><ul><li><a href="#accuracies-confusion-matrix" class="table-of-contents__link toc-highlight">Accuracies Confusion Matrix</a></li><li><a href="#predictions-confusion-matrix" class="table-of-contents__link toc-highlight">Predictions Confusion Matrix</a></li></ul></li></ul></div></div></div></div></div></div>
</body>
</html>